# 4. 모델 앙상블
class ModelEnsemble:
    def __init__(self, models):
        self.models = models
        
    def predict(self, x):
        predictions = []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(x)
                predictions.append(pred)
        
        # 평균 앙상블
        ensemble_pred = torch.stack(predictions).mean(dim=0)
        return ensemble_pred

# 5. 실시간 추론을 위한 최적화
def optimize_model_for_inference(model, input_shape):
    """모델을 추론 최적화"""
    model.eval()
    
    # TorchScript로 변환
    dummy_input = torch.randn(input_shape)
    traced_model = torch.jit.trace(model, dummy_input)
    
    # 추가 최적화
    traced_model = torch.jit.optimize_for_inference(traced_model)
    
    return traced_model

# 6. 모델 성능 측정
def measure_inference_time(model, input_tensor, num_runs=100):
    """추론 시간 측정"""
    model.eval()
    
    # GPU 워밍업
    for _ in range(10):
        _ = model(input_tensor)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    
    start_time = time.time()
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(input_tensor)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs
    fps = 1.0 / avg_time
    
    print(f"Average inference time: {avg_time*1000:.2f} ms")
    print(f"FPS: {fps:.2f}")
    
    return avg_time, fps
```

---

## 5. 최신 Transformer 기반 모델들

### Vision Transformer (ViT) 구현

```python
import torch
import torch.nn as nn
import math

class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        # 패치를 임베딩으로 변환
        self.projection = nn.Conv2d(
            in_channels, embed_dim, 
            kernel_size=patch_size, stride=patch_size
        )
        
    def forward(self, x):
        # x: [batch_size, channels, height, width]
        x = self.projection(x)  # [batch_size, embed_dim, n_patches^0.5, n_patches^0.5]
        x = x.flatten(2)        # [batch_size, embed_dim, n_patches]
        x = x.transpose(1, 2)   # [batch_size, n_patches, embed_dim]
        return x

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim=768, num_heads=12, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert embed_dim % num_heads == 0
        
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.attention_dropout = nn.Dropout(dropout)
        self.projection = nn.Linear(embed_dim, embed_dim)
        self.projection_dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        batch_size, seq_len, embed_dim = x.shape
        
        # QKV 계산
        qkv = self.qkv(x)  # [batch_size, seq_len, embed_dim * 3]
        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch_size, num_heads, seq_len, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Scaled Dot-Product Attention
        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_probs = torch.softmax(attention_scores, dim=-1)
        attention_probs = self.attention_dropout(attention_probs)
        
        # Apply attention to values
        context = torch.matmul(attention_probs, v)
        context = context.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)
        
        # Final projection
        output = self.projection(context)
        output = self.projection_dropout(output)
        
        return output

class TransformerEncoderLayer(nn.Module):
    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attention = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        
        self.norm2 = nn.LayerNorm(embed_dim)
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Self-attention with residual connection
        x = x + self.attention(self.norm1(x))
        
        # MLP with residual connection
        x = x + self.mlp(self.norm2(x))
        
        return x

class VisionTransformer(nn.Module):
    def __init__(
        self, 
        img_size=224, 
        patch_size=16, 
        in_channels=3,
        num_classes=1000,
        embed_dim=768,
        num_layers=12,
        num_heads=12,
        mlp_ratio=4,
        dropout=0.1
    ):
        super().__init__()
        
        # Patch embedding
        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        
        # Class token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Positional embedding
        num_patches = self.patch_embedding.n_patches
        self.positional_embedding = nn.Parameter(
            torch.zeros(1, num_patches + 1, embed_dim)
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # Transformer encoder layers
        self.transformer_layers = nn.ModuleList([
            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(num_layers)
        ])
        
        # Classification head
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.trunc_normal_(module.weight, std=0.02)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
    
    def forward(self, x):
        batch_size = x.shape[0]
        
        # Patch embedding
        x = self.patch_embedding(x)  # [batch_size, n_patches, embed_dim]
        
        # Add class token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # [batch_size, n_patches + 1, embed_dim]
        
        # Add positional embedding
        x = x + self.positional_embedding
        x = self.dropout(x)
        
        # Apply transformer layers
        for layer in self.transformer_layers:
            x = layer(x)
        
        # Classification
        x = self.norm(x)
        cls_token_final = x[:, 0]  # Use class token for classification
        output = self.head(cls_token_final)
        
        return output

# ViT 사용 예시
def create_vit_model(num_classes=10):
    model = VisionTransformer(
        img_size=224,
        patch_size=16,
        num_classes=num_classes,
        embed_dim=768,
        num_layers=12,
        num_heads=12
    )
    return model
```

### DETR (Detection Transformer) 구현

```python
class DETR(nn.Module):
    def __init__(self, num_classes=91, num_queries=100, hidden_dim=256):
        super().__init__()
        self.num_queries = num_queries
        
        # CNN Backbone (ResNet-50)
        self.backbone = models.resnet50(pretrained=True)
        self.backbone.fc = nn.Identity()  # Remove final FC layer
        
        # Reduce channel dimension
        self.input_proj = nn.Conv2d(2048, hidden_dim, kernel_size=1)
        
        # Transformer
        self.transformer = nn.Transformer(
            d_model=hidden_dim,
            nhead=8,
            num_encoder_layers=6,
            num_decoder_layers=6,
            dim_feedforward=2048
        )
        
        # Object queries
        self.query_embed = nn.Embedding(num_queries, hidden_dim)
        
        # Prediction heads
        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for "no object"
        self.bbox_embed = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 4)
        )
    
    def forward(self, x):
        # Backbone feature extraction
        features = self.backbone.conv1(x)
        features = self.backbone.bn1(features)
        features = self.backbone.relu(features)
        features = self.backbone.maxpool(features)
        
        features = self.backbone.layer1(features)
        features = self.backbone.layer2(features)
        features = self.backbone.layer3(features)
        features = self.backbone.layer4(features)
        
        # Reduce channels and flatten
        features = self.input_proj(features)  # [batch, hidden_dim, H, W]
        batch_size, _, H, W = features.shape
        features = features.flatten(2).permute(2, 0, 1)  # [H*W, batch, hidden_dim]
        
        # Positional encoding
        pos_embed = self.get_positional_encoding(batch_size, H, W, features.device)
        
        # Object queries
        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, batch_size, 1)
        
        # Transformer
        tgt = torch.zeros_like(query_embed)
        memory = self.transformer.encoder(features + pos_embed)
        hs = self.transformer.decoder(tgt, memory, 
                                    query_pos=query_embed, 
                                    pos=pos_embed)
        
        # Predictions
        outputs_class = self.class_embed(hs)
        outputs_coord = self.bbox_embed(hs).sigmoid()
        
        return {
            'pred_logits': outputs_class[-1],  # [batch, num_queries, num_classes + 1]
            'pred_boxes': outputs_coord[-1]    # [batch, num_queries, 4]
        }
    
    def get_positional_encoding(self, batch_size, H, W, device):
        # 2D positional encoding
        y_embed = torch.arange(H, device=device).float()
        x_embed = torch.arange(W, device=device).float()
        
        y_embed = y_embed / H
        x_embed = x_embed / W
        
        dim_t = torch.arange(128, device=device).float()
        dim_t = 10000 ** (2 * (dim_t // 2) / 128)
        
        pos_x = x_embed[:, None] / dim_t
        pos_y = y_embed[:, None] / dim_t
        
        pos_x = torch.stack((pos_x[:, 0::2].sin(), pos_x[:, 1::2].cos()), dim=2).flatten(1)
        pos_y = torch.stack((pos_y[:, 0::2].sin(), pos_y[:, 1::2].cos()), dim=2).flatten(1)
        
        pos = torch.cat((pos_y[:, None, :].repeat(1, W, 1),
                        pos_x[None, :, :].repeat(H, 1, 1)), dim=-1)
        
        return pos.flatten(0, 1).unsqueeze(1).repeat(1, batch_size, 1)

# DETR Loss 함수
class DETRLoss(nn.Module):
    def __init__(self, num_classes, weight_dict={'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}):
        super().__init__()
        self.num_classes = num_classes
        self.weight_dict = weight_dict
        
        # Hungarian matching을 위한 cost matrix 계산
        self.matcher = HungarianMatcher()
        
    def forward(self, outputs, targets):
        # Hungarian matching으로 예측과 GT 매칭
        indices = self.matcher(outputs, targets)
        
        # Classification loss
        loss_ce = self.loss_labels(outputs, targets, indices)
        
        # Bounding box loss
        loss_bbox = self.loss_boxes(outputs, targets, indices)
        
        # GIoU loss
        loss_giou = self.loss_giou(outputs, targets, indices)
        
        losses = {
            'loss_ce': loss_ce,
            'loss_bbox': loss_bbox,
            'loss_giou': loss_giou
        }
        
        # 가중치 적용
        total_loss = sum(losses[k] * self.weight_dict[k] for k in losses.keys())
        
        return total_loss, losses
```

---

## 6. 모델 배포 및 실무 적용

### TensorRT를 이용한 추론 최적화

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTInference:
    def __init__(self, engine_path):
        # TensorRT 엔진 로드
        self.logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, 'rb') as f:
            runtime = trt.Runtime(self.logger)
            self.engine = runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # 입력/출력 바인딩 설정
        self.inputs = []
        self.outputs = []
        self.bindings = []
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            # GPU 메모리 할당
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})
    
    def infer(self, input_data):
        # 입력 데이터를 GPU로 복사
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        
        stream = cuda.Stream()
        
        # GPU로 전송
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], stream)
        
        # 추론 실행
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=stream.handle)
        
        # 결과를 CPU로 복사
        cuda.memcpy_dtoh_async(self.outputs[0]['host'], self.outputs[0]['device'], stream)
        stream.synchronize()
        
        return self.outputs[0]['host']

def convert_pytorch_to_tensorrt(pytorch_model, input_shape, save_path):
    """PyTorch 모델을 TensorRT로 변환"""
    import torch2trt
    
    # 더미 입력 생성
    dummy_input = torch.randn(input_shape).cuda()
    
    # TensorRT로 변환
    model_trt = torch2trt.torch2trt(pytorch_model, [dummy_input])
    
    # 저장
    torch.save(model_trt.state_dict(), save_path)
    
    return model_trt
```

### Flask를 이용한 웹 API 구현

```python
from flask import Flask, request, jsonify
import cv2
import numpy as np
import base64
from PIL import Image
import io

app = Flask(__name__)

# 모델 로드 (전역 변수)
model = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def load_model():
    global model
    model = YOLOv1(num_classes=20)  # 또는 다른 모델
    model.load_state_dict(torch.load('best_model.pth', map_location=device))
    model.to(device)
    model.eval()

def preprocess_image(image_data):
    """이미지 전처리"""
    # Base64 디코딩
    image_data = base64.b64decode(image_data)
    image = Image.open(io.BytesIO(image_data))
    
    # RGB 변환
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # 리사이즈 및 정규화
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    image_tensor = transform(image).unsqueeze(0)
    return image_tensor

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        image_data = data['image']
        
        # 이미지 전처리
        input_tensor = preprocess_image(image_data)
        input_tensor = input_tensor.to(device)
        
        # 추론
        with torch.no_grad():
            outputs = model(input_tensor)
            
            # 후처리 (모델에 따라 달라짐)
            if isinstance(outputs, torch.Tensor):  # Classification
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted_class = torch.max(probabilities, 1)
                
                result = {
                    'predicted_class': predicted_class.item(),
                    'confidence': confidence.item(),
                    'probabilities': probabilities[0].cpu().numpy().tolist()
                }
            else:  # Object Detection
                # YOLO 출력 처리
                detections = post_process_yolo(outputs)
                result = {
                    'detections': detections
                }
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    load_model()
    app.run(host='0.0.0.0', port=5000, debug=False)
```

### 실시간 스트리밍 처리

```python
import cv2
import threading
import queue
import time

class RealTimeDetector:
    def __init__(self, model, confidence_threshold=0.5):
        self.model = model
        self.confidence_threshold = confidence_threshold
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # 프레임 버퍼
        self.frame_queue = queue.Queue(maxsize=10)
        self.result_queue = queue.Queue(maxsize=10)
        
        # 통계
        self.fps_counter = 0
        self.start_time = time.time()
        
    def capture_frames(self, video_source=0):
        """웹캠에서 프레임 캡처"""
        cap = cv2.VideoCapture(video_source)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 큐가 가득 찬 경우 오래된 프레임 제거
            if self.frame_queue.full():
                try:
                    self.frame_queue.get_nowait()
                except queue.Empty:
                    pass
            
            try:
                self.frame_queue.put(frame, block=False)
            except queue.Full:
                pass
        
        cap.release()
    
    def process_frames(self):
        """프레임 처리 및 추론"""
        while True:
            try:
                frame = self.frame_queue.get(timeout=1)
            except queue.Empty:
                continue
            
            # 전처리
            input_tensor = self.preprocess_frame(frame)
            
            # 추론
            with torch.no_grad():
                start_time = time.time()
                outputs = self.model(input_tensor)
                inference_time = time.time() - start_time
            
            # 후처리
            detections = self.postprocess_outputs(outputs, frame.shape)
            
            # 결과 저장
            result = {
                'frame': frame,
                'detections': detections,
                'inference_time': inference_time
            }
            
            if self.result_queue.full():
                try:
                    self.result_queue.get_nowait()
                except queue.Empty:
                    pass
            
            try:
                self.result_queue.put(result, block=False)
            except queue.Full:
                pass
    
    def preprocess_frame(self, frame):
        """프레임 전처리"""
        # BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # 리사이즈
        frame_resized = cv2.resize(frame_rgb, (416, 416))
        
        # 정규화 및 텐서 변환
        frame_tensor = torch.from_numpy(frame_resized).float()
        frame_tensor = frame_tensor.permute(2, 0, 1) / 255.0
        frame_tensor = frame_tensor.unsqueeze(0).to(self.device)
        
        return frame_tensor
    
    def postprocess_outputs(self, outputs, original_shape):
        """YOLO 출력 후처리"""
        # NMS 적용
        detections = []
        
        # 여기서 실제 YOLO 후처리 로직 구현
        # (confidence filtering, NMS 등)
        
        return detections
    
    def visualize_results(self):
        """결과 시각화"""
        while True:
            try:
                result = self.result_queue.get(timeout=1)
            except queue.Empty:
                continue
            
            frame = result['frame']
            detections = result['detections']
            
            # 바운딩 박스 그리기
            for detection in detections:
                x1, y1, x2, y2 = detection['bbox']
                confidence = detection['confidence']
                class_name = detection['class_name']
                
                # 바운딩 박스
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # 라벨
                label = f"{class_name}: {confidence:.2f}"
                cv2.putText(frame, label, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # FPS 표시
            self.fps_counter += 1
            if self.fps_counter % 30 == 0:
                elapsed_time = time.time() - self.start_time
                fps = self.fps_counter / elapsed_time
                cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            # 추론 시간 표시
            inference_time = result['inference_time']
            cv2.putText(frame, f"Inference: {inference_time*1000:.1f}ms", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            
            # 화면 출력
            cv2.imshow('Real-time Detection', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cv2.destroyAllWindows()
    
    def start(self, video_source=0):
        """멀티스레딩으로 실시간 처리 시작"""
        # 스레드 생성
        capture_thread = threading.Thread(target=self.capture_frames, args=(video_source,))
        process_thread = threading.Thread(target=self.process_frames)
        
        # 데몬 스레드로 설정
        capture_thread.daemon = True
        process_thread.daemon = True
        
        # 스레드 시작
        capture_thread.start()
        process_thread.start()
        
        # 메인 스레드에서 시각화
        self.visualize_results()

# 사용 예시
if __name__ == "__main__":
    # 모델 로드
    model = YOLOv1(num_classes=20)
    model.load_state_dict(torch.load('yolo_model.pth'))
    model.eval()
    
    # 실시간 감지기 생성 및 실행
    detector = RealTimeDetector(model)
    detector.start(video_source=0)  # 웹캠 사용
```

---

## 7. 성능 최적화 및 디버깅

### 메모리 효율적인 학습

```python
import torch.utils.checkpoint as checkpoint
from torch.cuda.amp import autocast, GradScaler

class MemoryEfficientTrainer:
    def __init__(self, model, use_mixed_precision=True, gradient_checkpointing=True):
        self.model = model
        self.use_mixed_precision = use_mixed_precision
        self.gradient_checkpointing = gradient_checkpointing
        
        if use_mixed_precision:
            self.scaler = GradScaler()
        
        if gradient_checkpointing:
            # Gradient checkpointing 활성화
            if hasattr(model, 'enable_gradient_checkpointing'):
                model.enable_gradient_checkpointing()
    
    def train_step(self, batch, criterion, optimizer):
        images, targets = batch
        
        # Mixed precision training
        if self.use_mixed_precision:
            with autocast():
                if self.gradient_checkpointing:
                    # Gradient checkpointing 사용
                    outputs = checkpoint.checkpoint(self.model, images)
                else:
                    outputs = self.model(images)
                
                loss = criterion(outputs, targets)
            
            # Backward pass with scaling
            self.scaler.scale(loss).backward()
            self.scaler.step(optimizer)
            self.scaler.update()
        else:
            # 일반 훈련
            outputs = self.model(images)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        
        optimizer.zero_grad()
        return loss.item()

# 메모리 사용량 모니터링
def monitor_memory_usage():
    """GPU 메모리 사용량 모니터링"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        cached = torch.cuda.memory_reserved() / 1024**3     # GB
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, "
              f"Cached: {cached:.2f}GB, Max: {max_allocated:.2f}GB")
        
        return allocated, cached, max_allocated
    return 0, 0, 0

# 배치 크기 자동 조정
def find_optimal_batch_size(model, input_shape, max_batch_size=512):
    """메모리에 맞는 최적 배치 크기 찾기"""
    model.train()
    device = next(model.parameters()).device
    
    batch_size = 1
    while batch_size <= max_batch_size:
        try:
            # 더미 데이터 생성
            dummy_input = torch.randn(batch_size, *input_shape[1:]).to(device)
            dummy_target = torch.randint(0, 10, (batch_size,)).to(device)
            
            # Forward pass
            outputs = model(dummy_input)
            loss = F.cross_entropy(outputs, dummy_target)
            
            # Backward pass
            loss.backward()
            
            # 메모리 정리
            del dummy_input, dummy_target, outputs, loss
            torch.cuda.empty_cache()
            
            print(f"Batch size {batch_size}: OK")
            batch_size *= 2
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                optimal_batch_size = batch_size // 2
                print(f"Optimal batch size: {optimal_batch_size}")
                return optimal_batch_size
            else:
                raise e
    
    return max_batch_size

# 모델 프로파일링
def profile_model(model, input_shape, num_runs=100):
    """모델 성능 프로파일링"""
    import time
    from torch.profiler import profile, record_function, ProfilerActivity
    
    model.eval()
    device = next(model.parameters()).device
    dummy_input = torch.randn(input_shape).to(device)
    
    # 워밍업
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # 프로파일링 실행
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                record_shapes=True) as prof:
        with record_function("model_inference"):
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = model(dummy_input)
    
    # 결과 출력
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    
    # Chrome tracing 파일로 저장
    prof.export_chrome_trace("model_trace.json")
```

---

## 8. 모델 해석 및 시각화

### Grad-CAM 구현

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt
from torch.autograd import grad

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        # Hook 등록
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)
    
    def save_activation(self, module, input, output):
        self.activations = output
    
    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]
    
    def generate_cam(self, input_tensor, class_idx=None):
        # Forward pass
        output = self.model(input_tensor)
        
        if class_idx is None:
            class_idx = output.argmax(dim=1)
        
        # Backward pass
        self.model.zero_grad()
        class_score = output[:, class_idx].squeeze()
        class_score.backward(retain_graph=True)
        
        # CAM 생성
        gradients = self.gradients
        activations = self.activations
        
        # Global Average Pooling of gradients
        weights = torch.mean(gradients, dim=[2, 3])
        
        # 가중합 계산
        cam = torch.zeros(activations.shape[2:], dtype=torch.float32)
        for i, w in enumerate(weights[0]):
            cam += w * activations[0, i, :, :]
        
        # ReLU 적용
        cam = torch.relu(cam)
        
        # 정규화
        cam = cam / torch.max(cam)
        
        return cam.detach().cpu().numpy()
    
    def visualize_cam(self, input_tensor, class_idx=None, alpha=0.4):
        # CAM 생성
        cam = self.generate_cam(input_tensor, class_idx)
        
        # 원본 이미지 준비
        image = input_tensor.squeeze().cpu().permute(1, 2, 0).numpy()
        image = (image - image.min()) / (image.max() - image.min())
        
        # CAM을 원본 이미지 크기로 리사이즈
        cam_resized = cv2.resize(cam, (image.shape[1], image.shape[0]))
        
        # 히트맵 생성
        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
        heatmap = heatmap / 255.0
        
        # 원본 이미지와 히트맵 결합
        cam_image = heatmap * alpha + image * (1 - alpha)
        
        return cam_image, cam_resized

# 사용 예시
def apply_gradcam(model, image_tensor, target_layer_name='layer4'):
    """GradCAM 적용 예시"""
    # 타겟 레이어 찾기
    target_layer = None
    for name, layer in model.named_modules():
        if name == target_layer_name:
            target_layer = layer
            break
    
    if target_layer is None:
        raise ValueError(f"Layer {target_layer_name} not found")
    
    # GradCAM 객체 생성
    gradcam = GradCAM(model, target_layer)
    
    # CAM 시각화
    cam_image, cam = gradcam.visualize_cam(image_tensor)
    
    # 결과 출력
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # 원본 이미지
    original = image_tensor.squeeze().cpu().permute(1, 2, 0).numpy()
    original = (original - original.min()) / (original.max() - original.min())
    axes[0].imshow(original)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # CAM 히트맵
    axes[1].imshow(cam, cmap='jet')
    axes[1].set_title('Grad-CAM')
    axes[1].axis('off')
    
    # 결합된 이미지
    axes[2].imshow(cam_image)
    axes[2].set_title('Grad-CAM Overlay')
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return cam_image, cam
```

### Feature Map 시각화

```python
def visualize_feature_maps(model, input_tensor, layer_names=None):
    """중간 레이어의 Feature Map 시각화"""
    
    activations = {}
    hooks = []
    
    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()
        return hook
    
    # Hook 등록
    if layer_names is None:
        layer_names = []
        for name, layer in model.named_modules():
            if isinstance(layer, (nn.Conv2d, nn.BatchNorm2d)):
                layer_names.append(name)
                hooks.append(layer.register_forward_hook(get_activation(name)))
    else:
        for name in layer_names:
            layer = dict(model.named_modules())[name]
            hooks.append(layer.register_forward_hook(get_activation(name)))
    
    # Forward pass
    model.eval()
    with torch.no_grad():
        _ = model(input_tensor)
    
    # Feature map 시각화
    for layer_name, activation in activations.items():
        # 첫 번째 배치, 처음 몇 개 채널만 선택
        feature_maps = activation[0]  # [C, H, W]
        num_channels = min(16, feature_maps.shape[0])
        
        fig, axes = plt.subplots(4, 4, figsize=(12, 12))
        fig.suptitle(f'Feature Maps - {layer_name}')
        
        for i in range(num_channels):
            row = i // 4
            col = i % 4
            
            feature_map = feature_maps[i].cpu().numpy()
            axes[row, col].imshow(feature_map, cmap='viridis')
            axes[row, col].set_title(f'Channel {i}')
            axes[row, col].axis('off')
        
        # 빈 subplot 숨기기
        for i in range(num_channels, 16):
            row = i // 4
            col = i % 4
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.show()
    
    # Hook 제거
    for hook in hooks:
        hook.remove()

# 모델 구조 시각화
def visualize_model_architecture(model, input_shape):
    """모델 아키텍처 시각화"""
    try:
        from torchviz import make_dot
        
        # 더미 입력으로 forward pass
        x = torch.randn(input_shape)
        y = model(x)
        
        # 계산 그래프 시각화
        dot = make_dot(y, params=dict(model.named_parameters()))
        dot.render('model_architecture', format='png')
        print("모델 아키텍처가 'model_architecture.png'로 저장되었습니다.")
        
    except ImportError:
        print("torchviz가 설치되지 않았습니다. 'pip install torchviz graphviz'로 설치해주세요.")

# 가중치 분포 시각화
def visualize_weight_distribution(model):
    """모델 가중치 분포 시각화"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    all_weights = []
    layer_weights = {}
    
    for name, param in model.named_parameters():
        if 'weight' in name and param.dim() > 1:
            weights = param.data.cpu().numpy().flatten()
            all_weights.extend(weights)
            layer_weights[name] = weights
    
    # 전체 가중치 분포
    axes[0, 0].hist(all_weights, bins=50, alpha=0.7, color='blue')
    axes[0, 0].set_title('전체 가중치 분포')
    axes[0, 0].set_xlabel('가중치 값')
    axes[0, 0].set_ylabel('빈도')
    
    # 레이어별 가중치 분포 (처음 4개 레이어)
    layer_names = list(layer_weights.keys())[:3]
    colors = ['red', 'green', 'orange']
    
    for i, (layer_name, color) in enumerate(zip(layer_names, colors)):
        if i < 3:
            row = (i + 1) // 2
            col = (i + 1) % 2
            axes[row, col].hist(layer_weights[layer_name], bins=30, 
                               alpha=0.7, color=color)
            axes[row, col].set_title(f'{layer_name} 가중치 분포')
            axes[row, col].set_xlabel('가중치 값')
            axes[row, col].set_ylabel('빈도')
    
    plt.tight_layout()
    plt.show()
```

---

## 9. 고급 기법 및 최신 연구

### Self-Supervised Learning 구현

```python
class SimCLR(nn.Module):
    """Simple Framework for Contrastive Learning of Visual Representations"""
    
    def __init__(self, base_encoder, projection_dim=128):
        super().__init__()
        
        # Backbone encoder
        self.encoder = base_encoder
        self.encoder.fc = nn.Identity()  # Remove classification head
        
        # Projection head
        self.projector = nn.Sequential(
            nn.Linear(512, 512),  # ResNet18의 경우
            nn.ReLU(),
            nn.Linear(512, projection_dim)
        )
    
    def forward(self, x):
        h = self.encoder(x)
        z = self.projector(h)
        return h, z

class NTXentLoss(nn.Module):
    """Normalized Temperature-scaled Cross Entropy Loss"""
    
    def __init__(self, temperature=0.5):
        super().__init__()
        self.temperature = temperature
        self.cosine_similarity = nn.CosineSimilarity(dim=-1)
    
    def forward(self, z_i, z_j):
        batch_size = z_i.shape[0]
        
        # L2 정규화
        z_i = F.normalize(z_i, p=2, dim=1)
        z_j = F.normalize(z_j, p=2, dim=1)
        
        # 양성 쌍들의 유사도
        pos_sim = self.cosine_similarity(z_i, z_j) / self.temperature
        
        # 모든 쌍들의 유사도
        z = torch.cat([z_i, z_j], dim=0)
        sim_matrix = torch.mm(z, z.t()) / self.temperature
        
        # 대각선 제거 (자기 자신과의 유사도)
        sim_matrix.fill_diagonal_(-float('inf'))
        
        # 손실 계산
        labels = torch.arange(batch_size).to(z_i.device)
        
        # i -> j 방향
        pos_sim_i_j = sim_matrix[labels, labels + batch_size]
        neg_sim_i = sim_matrix[labels, :]
        loss_i_j = -pos_sim_i_j + torch.logsumexp(neg_sim_i, dim=1)
        
        # j -> i 방향
        pos_sim_j_i = sim_matrix[labels + batch_size, labels]
        neg_sim_j = sim_matrix[labels + batch_size, :]
        loss_j_i = -pos_sim_j_i + torch.logsumexp(neg_sim_j, dim=1)
        
        return (loss_i_j + loss_j_i).mean()

# Data Augmentation for SimCLR
def get_simclr_augmentation():
    """SimCLR을 위한 강한 데이터 증강"""
    return transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomApply([
            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
        ], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])

def train_simclr(model, dataloader, criterion, optimizer, device, epochs=100):
    """SimCLR 학습 함수"""
    model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        
        for batch_idx, (images, _) in enumerate(dataloader):
            # 두 가지 다른 augmentation 적용
            aug_transform = get_simclr_augmentation()
            
            images_i = torch.stack([aug_transform(img) for img in images])
            images_j = torch.stack([aug_transform(img) for img in images])
            
            images_i = images_i.to(device)
            images_j = images_j.to(device)
            
            # Forward pass
            _, z_i = model(images_i)
            _, z_j = model(images_j)
            
            # Loss 계산
            loss = criterion(z_i, z_j)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        print(f'Epoch {epoch} Average Loss: {total_loss/len(dataloader):.4f}')
```

### Knowledge Distillation 구현

```python
class DistillationLoss(nn.Module):
    """Knowledge Distillation Loss"""
    
    def __init__(self, alpha=0.7, temperature=4):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, student_outputs, teacher_outputs, labels):
        # Soft targets (knowledge distillation)
        soft_teacher = F.softmax(teacher_outputs / self.temperature, dim=1)
        soft_student = F.log_softmax(student_outputs / self.temperature, dim=1)
        
        kd_loss = self.kl_div(soft_student, soft_teacher) * (self.temperature ** 2)
        
        # Hard targets (original task)
        ce_loss = self.ce_loss(student_outputs, labels)
        
        # 가중 결합
        total_loss = self.alpha * kd_loss + (1 - self.alpha) * ce_loss
        
        return total_loss, kd_loss, ce_loss

def train_with_distillation(teacher_model, student_model, dataloader, 
                           criterion, optimizer, device, epochs=50):
    """Knowledge Distillation을 이용한 학습"""
    
    teacher_model.eval()  # Teacher는 고정
    student_model.train()
    
    for epoch in range(epochs):
        total_loss = 0
        total_kd_loss = 0
        total_ce_loss = 0
        
        for batch_idx, (images, labels) in enumerate(dataloader):
            images = images.to(device)
            labels = labels.to(device)
            
            # Teacher 예측 (gradient 계산 안함)
            with torch.no_grad():
                teacher_outputs = teacher_model(images)
            
            # Student 예측
            student_outputs = student_model(images)
            
            # Loss 계산
            loss, kd_loss, ce_loss = criterion(student_outputs, teacher_outputs, labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_kd_loss += kd_loss.item()
            total_ce_loss += ce_loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}')
                print(f'Total Loss: {loss.item():.4f}, '
                      f'KD Loss: {kd_loss.item():.4f}, '
                      f'CE Loss: {ce_loss.item():.4f}')
        
        avg_loss = total_loss / len(dataloader)
        avg_kd_loss = total_kd_loss / len(dataloader)
        avg_ce_loss = total_ce_loss / len(dataloader)
        
        print(f'Epoch {epoch} - Total: {avg_loss:.4f}, '
              f'KD: {avg_kd_loss:.4f}, CE: {avg_ce_loss:.4f}')
```

### Test Time Augmentation (TTA)

```python
class TestTimeAugmentation:
    """Test Time Augmentation을 통한 예측 개선"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.model.eval()
    
    def get_tta_transforms(self):
        """TTA를 위한 변환들"""
        return [
            transforms.Compose([]),  # Original
            transforms.Compose([transforms.RandomHorizontalFlip(p=1.0)]),
            transforms.Compose([transforms.RandomVerticalFlip(p=1.0)]),
            transforms.Compose([
                transforms.RandomHorizontalFlip(p=1.0),
                transforms.RandomVerticalFlip(p=1.0)
            ]),
            transforms.Compose([transforms.RandomRotation((90, 90))]),
            transforms.Compose([transforms.RandomRotation((180, 180))]),
            transforms.Compose([transforms.RandomRotation((270, 270))]),
        ]
    
    def predict_with_tta(self, image_tensor, num_classes):
        """TTA를 적용한 예측"""
        transforms_list = self.get_tta_transforms()
        all_predictions = []
        
        with torch.no_grad():
            for transform in transforms_list:
                # 변환 적용
                if len(transform.transforms) > 0:
                    # PIL 이미지로 변환 후 다시 텐서로
                    pil_image = transforms.ToPILImage()(image_tensor.squeeze())
                    augmented_tensor = transform(pil_image)
                    if len(augmented_tensor.shape) == 3:
                        augmented_tensor = augmented_tensor.unsqueeze(0)
                else:
                    augmented_tensor = image_tensor
                
                augmented_tensor = augmented_tensor.to(self.device)
                
                # 예측
                outputs = self.model(augmented_tensor)
                predictions = F.softmax(outputs, dim=1)
                all_predictions.append(predictions)
        
        # 평균 앙상블
        final_prediction = torch.stack(all_predictions).mean(dim=0)
        
        return final_prediction
    
    def predict_batch_with_tta(self, dataloader):
        """배치 단위 TTA 예측"""
        all_predictions = []
        all_labels = []
        
        for images, labels in dataloader:
            batch_predictions = []
            
            for i in range(images.size(0)):
                single_image = images[i:i+1]
                tta_prediction = self.predict_with_tta(single_image, labels.max().item() + 1)
                batch_predictions.append(tta_prediction)
            
            batch_predictions = torch.cat(batch_predictions, dim=0)
            all_predictions.append(batch_predictions)
            all_labels.append(labels)
        
        final_predictions = torch.cat(all_predictions, dim=0)
        final_labels = torch.cat(all_labels, dim=0)
        
        return final_predictions, final_labels

# 사용 예시
def evaluate_with_tta(model, test_loader, device):
    """TTA를 사용한 모델 평가"""
    tta = TestTimeAugmentation(model, device)
    
    all_preds = []
    all_labels = []
    
    for images, labels in test_loader:
        images = images.to(device)
        
        # 배치의 각 이미지에 대해 TTA 적용
        batch_preds = []
        for i in range(images.size(0)):
            single_image = images[i:i+1]
            tta_pred = tta.predict_with_tta(single_image, 10)  # 10 classes
            batch_preds.append(tta_pred)
        
        batch_preds = torch.cat(batch_preds, dim=0)
        all_preds.append(batch_preds)
        all_labels.append(labels)
    
    # 전체 예측 결합
    final_preds = torch.cat(all_preds, dim=0)
    final_labels = torch.cat(all_labels, dim=0).to(device)
    
    # 정확도 계산
    predicted_classes = final_preds.argmax(dim=1)
    accuracy = (predicted_classes == final_labels).float().mean()
    
    print(f'TTA Accuracy: {accuracy.item():.4f}')
    
    return accuracy.item()
```

---

## 10. 종합 실습 프로젝트

### 멀티태스크 학습 프로젝트

```python
class MultiTaskModel(nn.Module):
    """Classification + Object Detection + Segmentation"""
    
    def __init__(self, num_classes_clf=10, num_classes_det=20, num_classes_seg=21):
        super().__init__()
        
        # 공유 백본
        self.backbone = models.resnet50(pretrained=True)
        self.backbone.fc = nn.Identity()
        
        # 공유 특징 추출기
        self.shared_features = nn.Sequential(
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes_clf)
        )
        
        # Detection head (단순화)
        self.detector = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, num_classes_det * 5)  # classes + bbox
        )
        
        # Segmentation head
        self.segmentation = nn.Sequential(
            nn.ConvTranspose2d(2048, 1024, 2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(1024, 512, 2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, 2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(256, num_classes_seg, 2, stride=2),
        )
    
    def forward(self, x, task='all'):
        # 백본을 통한 특징 추출
        features = self.backbone.conv1(x)
        features = self.backbone.bn1(features)
        features = self.backbone.relu(features)
        features = self.backbone.maxpool(features)
        
        features = self.backbone.layer1(features)
        features = self.backbone.layer2(features)
        features = self.backbone.layer3(features)
        layer4_features = self.backbone.layer4(features)
        
        # Global Average Pooling for classification/detection
        pooled_features = F.adaptive_avg_pool2d(layer4_features, (1, 1))
        pooled_features = pooled_features.view(pooled_features.size(0), -1)
        shared_features = self.shared_features(pooled_features)
        
        outputs = {}
        
        if task in ['all', 'classification']:
            outputs['classification'] = self.classifier(shared_features)
        
        if task in ['all', 'detection']:
            outputs['detection'] = self.detector(shared_features)
        
        if task in ['all', 'segmentation']:
            outputs['segmentation'] = self.segmentation(layer4_features)
        
        return outputs

class MultiTaskLoss(nn.Module):
    """멀티태스크 학습을 위한 손실 함수"""
    
    def __init__(self, task_weights={'clf': 1.0, 'det': 1.0, 'seg': 1.0}):
        super().__init__()
        self.task_weights = task_weights
        self.clf_loss = nn.CrossEntropyLoss()
        self.det_loss = nn.MSELoss()  # 단순화
        self.seg_loss = nn.CrossEntropyLoss()
    
    def forward(self, outputs, targets):
        total_loss = 0
        losses = {}
        
        if 'classification' in outputs and 'clf_labels' in targets:
            clf_loss = self.clf_loss(outputs['classification'], targets['clf_labels'])
            losses['clf_loss'] = clf_loss
            total_loss += self.task_weights['clf'] * clf_loss
        
        if 'detection' in outputs and 'det_labels' in targets:
            det_loss = self.det_loss(outputs['detection'], targets['det_labels'])
            losses['det_loss'] = det_loss
            total_loss += self.task_weights['det'] * det_loss
        
        if 'segmentation' in outputs and 'seg_labels' in targets:
            seg_loss = self.seg_loss(outputs['segmentation'], targets['seg_labels'])
            losses['seg_loss'] = seg_loss
            total_loss += self.task_weights['seg'] * seg_loss
        
        losses['total_loss'] = total_loss
        return total_loss, losses

def train_multitask_model():
    """멀티태스크 모델 학습 예시"""
    
    # 모델 초기화
    model = MultiTaskModel()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # 손실 함수 및 옵티마이저
    criterion = MultiTaskLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 학습 루프
    model.train()
    for epoch in range(100):
        for batch_idx, batch_data in enumerate(dataloader):
            images = batch_data['images'].to(device)
            
            # 타겟 준비
            targets = {
                'clf_labels': batch_data['clf_labels'].to(device),
                'det_labels': batch_data['det_labels'].to(device),
                'seg_labels': batch_data['seg_labels'].to(device)
            }
            
            # Forward pass
            outputs = model(images, task='all')
            
            # Loss 계산
            total_loss, individual_losses = criterion(outputs, targets)
            
            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # 로깅
            if batch_idx % 50 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}')
                print(f'Total Loss: {total_loss.item():.4f}')
                for task, loss in individual_losses.items():
                    if task != 'total_loss':
                        print(f'{task}: {loss.item():.4f}')
                print('-' * 50)

# 동적 가중치 조정 (Uncertainty weighting)
class UncertaintyWeighting(nn.Module):
    """태스크 불확실성 기반 가중치 자동 조정"""
    
    def __init__(self, num_tasks=3):
        super().__init__()
        # 각 태스크의 불확실성 파라미터 (학습 가능)
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))
    
    def forward(self, *losses):
        """
        Multi-task loss with uncertainty weighting
        L = Σ (1/(2σ²)) * L_i + log(σ)
        """
        total_loss = 0
        for i, loss in enumerate(losses):
            precision = torch.exp(-self.log_vars[i])
            total_loss += precision * loss + self.log_vars[i]
        
        return total_loss

# 사용 예시
def train_with_uncertainty_weighting():
    model = MultiTaskModel()
    uncertainty_weighting = UncertaintyWeighting(num_tasks=3)
    
    # 모든 파라미터를 하나의 옵티마이저로
    params = list(model.parameters()) + list(uncertainty_weighting.parameters())
    optimizer = optim.Adam(params, lr=0.001)
    
    for epoch in range(100):
        for batch_data in dataloader:
            # ... 데이터 준비 ...
            
            outputs = model(images)
            
            # 개별 손실 계산
            clf_loss = F.cross_entropy(outputs['classification'], targets['clf_labels'])
            det_loss = F.mse_loss(outputs['detection'], targets['det_labels'])
            seg_loss = F.cross_entropy(outputs['segmentation'], targets['seg_labels'])
            
            # 불확실성 가중치 적용
            total_loss = uncertainty_weighting(clf_loss, det_loss, seg_loss)
            
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # 가중치 출력
            if batch_idx % 100 == 0:
                weights = torch.exp(-uncertainty_weighting.log_vars)
                print(f'Task weights: {weights.detach().cpu().numpy()}')
```

---

## 11. 최종 프로젝트: 통합 컴퓨터 비전 시스템

### 실시간 멀티모달 분석 시스템

```python
import asyncio
import websockets
import json
import base64
from concurrent.futures import ThreadPoolExecutor

class UnifiedVisionSystem:
    """통합 컴퓨터 비전 시스템"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # 모델들 로드
        self.classification_model = self.load_classification_model()
        self.detection_model = self.load_detection_model()
        self.segmentation_model = self.load_segmentation_model()
        
        # 전처리 파이프라인
        self.preprocess = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # 클래스 이름들
        self.class_names = self.load_class_names()
        
        # 스레드 풀
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    def load_classification_model(self):
        """분류 모델 로드"""
        model = models.resnet50(pretrained=True)
        model.eval()
        return model.to(self.device)
    
    def load_detection_model(self):
        """객체 탐지 모델 로드"""
        model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
        model.eval()
        return model.to(self.device)
    
    def load_segmentation_model(self):
        """분할 모델 로드"""
        model = UNet(n_channels=3, n_classes=21)  # 이전에 정의한 U-Net
        # model.load_state_dict(torch.load('segmentation_model.pth'))
        model.eval()
        return model.to(self.device)
    
    def load_class_names(self):
        """클래스 이름 로드"""
        return {
            'classification': [f'class_{i}' for i in range(1000)],  # ImageNet classes
            'detection': [f'object_{i}' for i in range(80)],        # COCO classes
            'segmentation': [f'segment_{i}' for i in range(21)]     # Pascal VOC classes
        }
    
    async def process_image(self, image_data, tasks=['classification']):
        """이미지 처리 메인 함수"""
        try:
            # 이미지 디코딩
            image = self.decode_image(image_data)
            
            # 병렬 처리를 위한 태스크 생성
            future_tasks = []
            
            if 'classification' in tasks:
                future_tasks.append(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor, self.classify_image, image
                    )
                )
            
            if 'detection' in tasks:
                future_tasks.append(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor, self.detect_objects, image
                    )
                )
            
            if 'segmentation' in tasks:
                future_tasks.append(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor, self.segment_image, image
                    )
                )
            
            # 모든 태스크 병렬 실행
            results = await asyncio.gather(*future_tasks)
            
            # 결과 조합
            final_result = {
                'success': True,
                'results': {}
            }
            
            for i, task in enumerate(['classification', 'detection', 'segmentation']):
                if task in tasks:
                    final_result['results'][task] = results[i]
            
            return final_result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def decode_image(self, image_data):
        """Base64 이미지 디코딩"""
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        return image
    
    def classify_image(self, image):
        """이미지 분류"""
        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            outputs = self.classification_model(input_tensor)
            probabilities = F.softmax(outputs, dim=1)
            
            # Top-5 예측
            top5_prob, top5_idx = torch.topk(probabilities, 5)
            
            results = []
            for i in range(5):
                results.append({
                    'class_id': top5_idx[0][i].item(),
                    'class_name': self.class_names['classification'][top5_idx[0][i]],
                    'confidence': top5_prob[0][i].item()
                })
        
        return {
            'task': 'classification',
            'predictions': results,
            'processing_time': 0.1  # 실제로는 시간 측정
        }
    
    def detect_objects(self, image):
        """객체 탐지"""
        # YOLOv5는 PIL 이미지를 직접 받을 수 있음
        results = self.detection_model(image)
        
        # 결과 파싱
        detections = []
        for *box, conf, cls in results.xyxy[0].cpu().numpy():
            if conf > 0.5:  # 임계값
                detections.append({
                    'bbox': [int(x) for x in box],  # [x1, y1, x2, y2]
                    'confidence': float(conf),
                    'class_id': int(cls),
                    'class_name': self.class_names['detection'][int(cls)]
                })
        
        return {
            'task': 'detection',
            'detections': detections,
            'processing_time': 0.2
        }
    
    def segment_image(self, image):
        """이미지 분할"""
        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            outputs = self.segmentation_model(input_tensor)
            predicted_mask = torch.argmax(outputs, dim=1).squeeze().cpu().numpy()
        
        # 마스크를 Base64로 인코딩 (시각화용)
        mask_image = Image.fromarray((predicted_mask * 255 / predicted_mask.max()).astype(np.uint8))
        buffer = io.BytesIO()
        mask_image.save(buffer, format='PNG')
        mask_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # 클래스별 픽셀 수 계산
        unique, counts = np.unique(predicted_mask, return_counts=True)
        class_statistics = []
        for cls, count in zip(unique, counts):
            class_statistics.append({
                'class_id': int(cls),
                'class_name': self.class_names['segmentation'][int(cls)],
                'pixel_count': int(count),
                'percentage': float(count / predicted_mask.size * 100)
            })
        
        return {
            'task': 'segmentation',
            'mask_base64': mask_base64,
            'class_statistics': class_statistics,
            'processing_time': 0.3
        }

# WebSocket 서버 구현
class VisionWebSocketServer:
    def __init__(self):
        self.vision_system = UnifiedVisionSystem()
    
    async def handle_client(self, websocket, path):
        """클라이언트 요청 처리"""
        print(f"새 클라이언트 연결: {websocket.remote_address}")
        
        try:
            async for message in websocket:
                # JSON 메시지 파싱
                data = json.loads(message)
                
                if data['type'] == 'image_analysis':
                    # 이미지 분석 요청 처리
                    image_data = data['image']
                    requested_tasks = data.get('tasks', ['classification'])
                    
                    # 분석 실행
                    result = await self.vision_system.process_image(
                        image_data, requested_tasks
                    )
                    
                    # 결과 전송
                    response = {
                        'type': 'analysis_result',
                        'request_id': data.get('request_id'),
                        'result': result
                    }
                    
                    await websocket.send(json.dumps(response))
                
                elif data['type'] == 'health_check':
                    # 헬스 체크
                    await websocket.send(json.dumps({
                        'type': 'health_response',
                        'status': 'healthy',
                        'timestamp': time.time()
                    }))
        
        except websockets.exceptions.ConnectionClosed:
            print(f"클라이언트 연결 종료: {websocket.remote_address}")
        except Exception as e:
            print(f"에러 발생: {e}")
            await websocket.send(json.dumps({
                'type': 'error',
                'message': str(e)
            }))
    
    def start_server(self, host='localhost', port=8765):
        """서버 시작"""
        print(f"서버 시작: ws://{host}:{port}")
        return websockets.serve(self.handle_client, host, port)

# 클라이언트 예시
class VisionClient:
    def __init__(self, server_url='ws://localhost:8765'):
        self.server_url = server_url
    
    async def analyze_image(self, image_path, tasks=['classification', 'detection']):
        """이미지 분석 요청"""
        # 이미지를 Base64로 인코딩
        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode()
        
        # WebSocket 연결
        async with websockets.connect(self.server_url) as websocket:
            # 분석 요청
            request = {
                'type': 'image_analysis',
                'image': image_data,
                'tasks': tasks,
                'request_id': str(time.time())
            }
            
            await websocket.send(json.dumps(request))
            
            # 결과 수신
            response = await websocket.recv()
            result = json.loads(response)
            
            return result
    
    async def health_check(self):
        """서버 상태 확인"""
        async with websockets.connect(self.server_url) as websocket:
            await websocket.send(json.dumps({'type': 'health_check'}))
            response = await websocket.recv()
            return json.loads(response)

# 벤치마크 및 성능 테스트
class PerformanceBenchmark:
    def __init__(self, vision_system):
        self.vision_system = vision_system
        self.results = {
            'classification': [],
            'detection': [],
            'segmentation': []
        }
    
    async def benchmark_task(self, task, test_images, num_runs=100):
        """특정 태스크 벤치마크"""
        print(f"벤치마킹 시작: {task}")
        
        times = []
        for i in range(num_runs):
            image = random.choice(test_images)
            
            start_time = time.time()
            result = await self.vision_system.process_image(image, [task])
            end_time = time.time()
            
            processing_time = end_time - start_time
            times.append(processing_time)
            
            if i % 10 == 0:
                print(f"Progress: {i+1}/{num_runs}")
        
        # 통계 계산
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        fps = 1.0 / avg_time
        
        benchmark_result = {
            'task': task,
            'avg_time': avg_time,
            'std_time': std_time,
            'min_time': min_time,
            'max_time': max_time,
            'fps': fps,
            'num_runs': num_runs
        }
        
        self.results[task] = benchmark_result
        
        print(f"{task} 벤치마크 결과:")
        print(f"  평균 시간: {avg_time:.4f}s")
        print(f"  표준편차: {std_time:.4f}s")
        print(f"  FPS: {fps:.2f}")
        print("-" * 50)
        
        return benchmark_result
    
    def save_benchmark_results(self, filepath='benchmark_results.json'):
        """벤치마크 결과 저장"""
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"벤치마크 결과 저장됨: {filepath}")

# 실행 예시
async def main():
    # 서버 시작
    server = VisionWebSocketServer()
    start_server = server.start_server()
    
    # 이벤트 루프에서 서버 실행
    await start_server
    
    # 무한 대기 (실제로는 다른 방식으로 서버 관리)
    await asyncio.Future()  # 무한 대기

# 클라이언트 사용 예시
async def client_example():
    client = VisionClient()
    
    # 헬스 체크
    health = await client.health_check()
    print("서버 상태:", health)
    
    # 이미지 분석
    result = await client.analyze_image(
        'test_image.jpg', 
        tasks=['classification', 'detection', 'segmentation']
    )
    
    print("분석 결과:")
    print(json.dumps(result, indent=2))

if __name__ == "__main__":
    # 서버 실행
    # asyncio.run(main())
    
    # 또는 클라이언트 테스트
    asyncio.run(client_example())
```

---

## 12. 최종 정리 및 학습 로드맵

### 핵심 개념 요약

```python
# 컴퓨터 비전 학습 체크리스트
COMPUTER_VISION_CHECKLIST = {
    "기초 개념": [
        "✓ Image Classification 이해 및 구현",
        "✓ Object Detection (1-stage vs 2-stage) 구현", 
        "✓ Image Segmentation (Semantic, Instance, Panoptic) 구현",
        "✓ CNN 아키텍처 발전사 이해",
        "✓ 전이학습 및 파인튜닝 적용"
    ],
    
    "고급 기법": [
        "✓ Transformer 기반 모델 (ViT, DETR) 구현",
        "✓ Self-supervised Learning 적용",
        "✓ Knowledge Distillation 구현",
        "✓ Test Time Augmentation 활용",
        "✓ 멀티태스크 학습 구현"
    ],
    
    "실무 기술": [
        "✓ 모델 최적화 (양자화, 프루닝, TensorRT)",
        "✓ 실시간 추론 파이프라인 구축",
        "✓ 웹 API 및 마이크로서비스 구현",
        "✓ 성능 모니터링 및 벤치마킹",
        "✓ 모델 해석 및 시각화"
    ],
    
    "최신 연구": [
        "✓ Foundation Models 이해",
        "✓ Multimodal Learning 개념",
        "✓ Neural Architecture Search",
        "✓ Few-shot Learning 적용",
        "✓ Continual Learning 구현"
    ]
}

def print_learning_roadmap():
    """학습 로드맵 출력"""
    
    roadmap = {
        "1단계 (1-2개월)": [
            "CNN 기초와 PyTorch 마스터",
            "CIFAR-10/100으로 분류기 구현",
            "전이학습으로 커스텀 데이터셋 분류",
            "데이터 증강 및 정규화 기법 실습"
        ],
        
        "2단계 (2-3개월)": [
            "YOLO로 객체 탐지 구현",
            "U-Net으로 의료 영상 분할",
            "평가 지표 (mAP, IoU, Dice) 이해",
            "모델 성능 최적화 기법 학습"
        ],
        
        "3단계 (3-4개월)": [
            "Vision Transformer 구현",
            "DETR로 앵커 없는 탐지 학습",
            "Self-supervised Learning 적용",
            "Knowledge Distillation 실습"
        ],
        
        "4단계 (4-6개월)": [
            "실시간 추론 시스템 구축",
            "모델 배포 및 서빙 (Flask, FastAPI)",
            "TensorRT 최적화 적용",
            "멀티태스크 학습 프로젝트"
        ],
        
        "5단계 (6개월+)": [
            "최신 논문 구현 및 재현",
            "오픈소스 프로젝트 기여",
            "연구 프로젝트 수행",
            "컴퓨터 비전 제품 개발"
        ]
    }
    
    for stage, tasks in roadmap.items():
        print(f"\n{stage}:")
        for task in tasks:
            print(f"  • {task}")
    
    print("\n" + "="*60)
    print("추천 실습 프로젝트:")
    print("  1. 개와 고양이 분류기 (전이학습)")
    print("  2. 얼굴 마스크 착용 감지기 (YOLO)")
    print("  3. 도로 주행 환경 분할 (U-Net)")
    print("  4. 실시간 객체 인식 웹앱")
    print("  5. 멀티모달 이미지 분석 시스템")

# 리소스 추천
RECOMMENDED_RESOURCES = {
    "온라인 강의": [
        "Stanford CS231n: Convolutional Neural Networks",
        "Fast.ai Practical Deep Learning for Coders",
        "Coursera Deep Learning Specialization"
    ],
    
    "논문": [
        "ImageNet Classification with Deep CNNs (AlexNet)",
        "Very Deep CNNs for Large-Scale Image Recognition (VGG)",
        "Deep Residual Learning for Image Recognition (ResNet)",
        "You Only Look Once: Unified, Real-Time Object Detection",
        "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "Attention Is All You Need (Transformer)",
        "An Image is Worth 16x16 Words (Vision Transformer)"
    ],
    
    "실습 환경": [
        "Google Colab (무료 GPU)",
        "Kaggle Kernels (무료 GPU/TPU)",
        "Papers With Code (논문 구현체)",
        "PyTorch Hub (사전훈련 모델)"
    ],
    
    "데이터셋": [
        "CIFAR-10/100 (이미지 분류)",
        "ImageNet (대규모 분류)",
        "MS COCO (객체 탐지/분할)", 
        "Pascal VOC (객체 탐지/분할)",
        "Cityscapes (자율주행 분할)",
        "Open Images (다양한 태스크)"
    ]
}

if __name__ == "__main__":
    print("🎯 컴퓨터 비전 마스터 로드맵")
    print_learning_roadmap()
    
    print(f"\n📚 추천 학습 리소스:")
    for category, resources in RECOMMENDED_RESOURCES.items():
        print(f"\n{category}:")
        for resource in resources:
            print(f"  • {resource}")
    
    print(f"\n🎉 축하합니다! 이제 컴퓨터 비전의 세계로 떠날 준비가 되었습니다!")
    print("꾸준한 실습과 프로젝트를 통해 실력을 키워나가세요! 💪")
```

이제 **완전한 컴퓨터 비전 학습 가이드**가 완성되었습니다! 

**주요 특징:**
- 📖 **이론부터 실습까지** 완전한 커버
- 💻 **실제 동작하는 코드** 예시 제공
- 🚀 **최신 기술**들 (Transformer, Self-supervised Learning 등) 포함
- 🏭 **실무 활용** 가능한 배포 및 최적화 기법
- 📈 **체계적인 학습 로드맵** 제시

이 가이드를 따라 단계별로 학습하시면, 컴퓨터 비전 분야의 전문가가 될 수 있을 것입니다! 🎯
             컴퓨터 비전 기초 개념 - 코드 실습 가이드

## 1. Image Classification (이미지 분류)

### 개념 및 정의
이미지 분류는 입력 이미지를 미리 정의된 카테고리 중 하나로 분류하는 작업입니다. 전체 이미지에 대해 단일 클래스 라벨을 예측하며, 물체의 위치 정보는 제공하지 않습니다.

### 기본 CNN 구현 예시

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 1. 기본 CNN 모델 정의
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # Feature extraction layers
        self.features = nn.Sequential(
            # Conv Block 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Conv Block 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Conv Block 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.classifier(x)
        return x

# 2. 데이터 전처리 및 로딩
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10('/data', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR10('/data', train=False, download=True, transform=transform_test)

trainloader = DataLoader(trainset, batch_size=128, shuffle=True)
testloader = DataLoader(testset, batch_size=100, shuffle=False)

# 3. 학습 함수
def train_model(model, trainloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        if batch_idx % 100 == 99:
            print(f'Batch {batch_idx+1}, Loss: {running_loss/100:.3f}, Acc: {100.*correct/total:.2f}%')
            running_loss = 0.0

# 4. 평가 함수
def evaluate_model(model, testloader, device):
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in testloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')
    return accuracy

# 5. 학습 실행
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleCNN(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습률 스케줄러
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# 학습 루프
num_epochs = 100
for epoch in range(num_epochs):
    print(f'\nEpoch {epoch+1}/{num_epochs}')
    train_model(model, trainloader, criterion, optimizer, device)
    evaluate_model(model, testloader, device)
    scheduler.step()
```

### Transfer Learning 구현

```python
import torchvision.models as models

# 1. 사전 훈련된 ResNet 사용
def create_transfer_learning_model(num_classes, freeze_features=True):
    # 사전 훈련된 ResNet18 로드
    model = models.resnet18(pretrained=True)
    
    # Feature extraction layers 동결 (선택사항)
    if freeze_features:
        for param in model.parameters():
            param.requires_grad = False
    
    # 마지막 FC layer 교체
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, num_classes)
    )
    
    return model

# 2. 사용 예시
model = create_transfer_learning_model(num_classes=10, freeze_features=True)

# Feature extraction의 경우 FC layer만 학습
if freeze_features:
    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
else:
    # Fine-tuning의 경우 전체 네트워크를 낮은 학습률로 학습
    optimizer = optim.Adam([
        {'params': model.fc.parameters(), 'lr': 0.001},
        {'params': model.layer4.parameters(), 'lr': 0.0001},
        {'params': model.layer3.parameters(), 'lr': 0.00001}
    ])
```

---

## 2. Object Detection (객체 탐지)

### YOLO 구현 예시

```python
import torch
import torch.nn as nn
import numpy as np

# 1. YOLO 네트워크 구조 (단순화된 버전)
class YOLOv1(nn.Module):
    def __init__(self, num_classes=20, num_boxes=2):
        super(YOLOv1, self).__init__()
        self.num_classes = num_classes
        self.num_boxes = num_boxes
        
        # Backbone (CNN feature extractor)
        self.backbone = nn.Sequential(
            # Conv layers (simplified)
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 192, 3, padding=1),
            nn.BatchNorm2d(192),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            # 추가 Conv layers...
            nn.Conv2d(192, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(512, 1024, 3, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
        )
        
        # Detection head
        # 출력: S × S × (B*5 + C) = 7 × 7 × (2*5 + 20) = 7 × 7 × 30
        self.detector = nn.Sequential(
            nn.Conv2d(1024, 1024, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, num_boxes * 5 + num_classes, 1)
        )
    
    def forward(self, x):
        x = self.backbone(x)
        x = self.detector(x)
        return x

# 2. YOLO Loss Function
class YOLOLoss(nn.Module):
    def __init__(self, num_classes=20, num_boxes=2, lambda_coord=5, lambda_noobj=0.5):
        super(YOLOLoss, self).__init__()
        self.num_classes = num_classes
        self.num_boxes = num_boxes
        self.lambda_coord = lambda_coord
        self.lambda_noobj = lambda_noobj
        
    def forward(self, predictions, targets):
        batch_size = predictions.size(0)
        S = 7  # Grid size
        
        # Reshape predictions: [batch, S, S, B*5+C]
        predictions = predictions.view(batch_size, S, S, self.num_boxes * 5 + self.num_classes)
        
        # 좌표 loss (객체가 있는 셀에 대해서만)
        coord_loss = 0
        confidence_loss = 0
        class_loss = 0
        
        for b in range(batch_size):
            for i in range(S):
                for j in range(S):
                    # 해당 셀에 객체가 있는지 확인
                    if targets[b, i, j, 4] == 1:  # 객체 존재
                        # 가장 높은 IoU를 가진 bounding box 선택
                        best_box = 0  # 단순화
                        
                        # 좌표 loss
                        pred_x = predictions[b, i, j, best_box * 5]
                        pred_y = predictions[b, i, j, best_box * 5 + 1]
                        pred_w = predictions[b, i, j, best_box * 5 + 2]
                        pred_h = predictions[b, i, j, best_box * 5 + 3]
                        
                        target_x = targets[b, i, j, 0]
                        target_y = targets[b, i, j, 1]
                        target_w = targets[b, i, j, 2]
                        target_h = targets[b, i, j, 3]
                        
                        coord_loss += (pred_x - target_x) ** 2
                        coord_loss += (pred_y - target_y) ** 2
                        coord_loss += (torch.sqrt(pred_w) - torch.sqrt(target_w)) ** 2
                        coord_loss += (torch.sqrt(pred_h) - torch.sqrt(target_h)) ** 2
        
        total_loss = self.lambda_coord * coord_loss + confidence_loss + class_loss
        return total_loss

# 3. IoU 계산 함수
def calculate_iou(box1, box2):
    """
    box format: [x_center, y_center, width, height]
    """
    # Convert to corner coordinates
    box1_x1 = box1[0] - box1[2] / 2
    box1_y1 = box1[1] - box1[3] / 2
    box1_x2 = box1[0] + box1[2] / 2
    box1_y2 = box1[1] + box1[3] / 2
    
    box2_x1 = box2[0] - box2[2] / 2
    box2_y1 = box2[1] - box2[3] / 2
    box2_x2 = box2[0] + box2[2] / 2
    box2_y2 = box2[1] + box2[3] / 2
    
    # Intersection coordinates
    inter_x1 = max(box1_x1, box2_x1)
    inter_y1 = max(box1_y1, box2_y1)
    inter_x2 = min(box1_x2, box2_x2)
    inter_y2 = min(box1_y2, box2_y2)
    
    if inter_x2 < inter_x1 or inter_y2 < inter_y1:
        return 0.0
    
    # Calculate areas
    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)
    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)
    
    union_area = box1_area + box2_area - inter_area
    
    return inter_area / union_area

# 4. Non-Maximum Suppression
def non_max_suppression(boxes, scores, threshold=0.5):
    """
    boxes: [N, 4] - (x1, y1, x2, y2)
    scores: [N] - confidence scores
    """
    if len(boxes) == 0:
        return []
    
    # Sort by scores in descending order
    indices = torch.argsort(scores, descending=True)
    
    keep = []
    while len(indices) > 0:
        # Keep the box with highest score
        current = indices[0]
        keep.append(current.item())
        
        if len(indices) == 1:
            break
        
        # Calculate IoU with remaining boxes
        current_box = boxes[current]
        remaining_boxes = boxes[indices[1:]]
        
        # Remove boxes with high IoU
        ious = torch.stack([calculate_iou(current_box, box) for box in remaining_boxes])
        indices = indices[1:][ious < threshold]
    
    return keep

# 5. 모델 사용 예시
model = YOLOv1(num_classes=20)
criterion = YOLOLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 예시
def train_yolo(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        images = images.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        predictions = model(images)
        loss = criterion(predictions, targets)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 100 == 0:
            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    return total_loss / len(dataloader)
```

---

## 3. Image Segmentation (이미지 분할)

### U-Net 구현

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 1. U-Net 모델 구현
class DoubleConv(nn.Module):
    """두 번의 3x3 convolution"""
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return self.double_conv(x)

class Down(nn.Module):
    """Downscaling with maxpool then double conv"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )
    
    def forward(self, x):
        return self.maxpool_conv(x)

class Up(nn.Module):
    """Upscaling then double conv"""
    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()
        
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)
    
    def forward(self, x1, x2):
        x1 = self.up(x1)
        
        # 크기 맞추기 (input size가 2의 배수가 아닐 때)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        
        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        
        # Skip connection
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
    
    def forward(self, x):
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear
        
        # Encoder (Contracting path)
        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        factor = 2 if bilinear else 1
        self.down4 = Down(512, 1024 // factor)
        
        # Decoder (Expansive path)
        self.up1 = Up(1024, 512 // factor, bilinear)
        self.up2 = Up(512, 256 // factor, bilinear)
        self.up3 = Up(256, 128 // factor, bilinear)
        self.up4 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_classes)
    
    def forward(self, x):
        # Encoder
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        
        # Decoder with skip connections
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits

# 2. Segmentation Loss Functions
class DiceLoss(nn.Module):
    def __init__(self, smooth=1):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    
    def forward(self, inputs, targets):
        # Flatten tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()
        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)
        
        return 1 - dice

class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.5):
        super(CombinedLoss, self).__init__()
        self.alpha = alpha
        self.dice_loss = DiceLoss()
        self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, inputs, targets):
        dice = self.dice_loss(torch.softmax(inputs, dim=1), targets)
        ce = self.ce_loss(inputs, targets.long())
        return self.alpha * dice + (1 - self.alpha) * ce

# 3. 평가 지표 계산
def calculate_iou_segmentation(pred, target, num_classes):
    """Segmentation을 위한 IoU 계산"""
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    
    for cls in range(num_classes):
        pred_cls = pred == cls
        target_cls = target == cls
        
        intersection = (pred_cls & target_cls).sum().float()
        union = (pred_cls | target_cls).sum().float()
        
        if union == 0:
            iou = 1.0  # 해당 클래스가 없는 경우
        else:
            iou = intersection / union
        
        ious.append(iou.item())
    
    return ious

# 4. 학습 및 평가 함수
def train_segmentation(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for batch_idx, (images, masks) in enumerate(dataloader):
        images = images.to(device)
        masks = masks.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 50 == 0:
            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    return total_loss / len(dataloader)

def evaluate_segmentation(model, dataloader, device, num_classes):
    model.eval()
    total_iou = [0] * num_classes
    num_samples = 0
    
    with torch.no_grad():
        for images, masks in dataloader:
            images = images.to(device)
            masks = masks.to(device)
            
            outputs = model(images)
            predictions = torch.argmax(outputs, dim=1)
            
            # 배치의 각 이미지에 대해 IoU 계산
            for i in range(images.size(0)):
                ious = calculate_iou_segmentation(predictions[i], masks[i], num_classes)
                for j, iou in enumerate(ious):
                    total_iou[j] += iou
                num_samples += 1
    
    # 평균 IoU 계산
    mean_ious = [iou / num_samples for iou in total_iou]
    miou = sum(mean_ious) / len(mean_ious)
    
    print(f'mIoU: {miou:.4f}')
    for i, iou in enumerate(mean_ious):
        print(f'Class {i} IoU: {iou:.4f}')
    
    return miou

# 5. 사용 예시
def main():
    # 모델 초기화
    model = UNet(n_channels=3, n_classes=2)  # 이진 분할
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Loss와 Optimizer
    criterion = CombinedLoss(alpha=0.5)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 데이터 로더 (가정)
    # train_loader, val_loader = get_data_loaders()
    
    # 학습 루프
    num_epochs = 100
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        
        # train_loss = train_segmentation(model, train_loader, criterion, optimizer, device)
        # miou = evaluate_segmentation(model, val_loader, device, num_classes=2)
        
        # print(f'Train Loss: {train_loss:.4f}, mIoU: {miou:.4f}')

# 6. 시각화 함수
import matplotlib.pyplot as plt

def visualize_segmentation(image, mask, prediction, save_path=None):
    """분할 결과 시각화"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # 원본 이미지
    axes[0].imshow(image.permute(1, 2, 0))
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # Ground Truth
    axes[1].imshow(mask, cmap='viridis')
    axes[1].set_title('Ground Truth')
    axes[1].axis('off')
    
    # Prediction
    axes[2].imshow(prediction, cmap='viridis')
    axes[2].set_title('Prediction')
    axes[2].axis('off')
    
    if save_path:
        plt.savefig(save_path)
    plt.show()
```

---

## 4. 실무 활용 예시

### 데이터 전처리 파이프라인

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2

# 1. 강력한 데이터 증강 파이프라인
def get_training_augmentation():
    train_transform = [
        # 기하학적 변환
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.2),
        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, 
                          rotate_limit=45, p=0.5),
        
        # 색상 변환
        A.OneOf([
            A.CLAHE(clip_limit=2),
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),
            A.RandomGamma()
        ], p=0.5),
        
        # 노이즈 추가
        A.OneOf([
            A.GaussNoise(),
            A.GaussianBlur(),
            A.MotionBlur()
        ], p=0.2),
        
        # 정규화 및 텐서 변환
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
    ]
    return A.Compose(train_transform)

# 2. 커스텀 데이터셋 클래스
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # 이미지 로드
        image = cv2.imread(self.image_paths[idx])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        label = self.labels[idx]
        
        # 변환 적용
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label

# 3. 모델 성능 모니터링
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
    
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
        
        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

#