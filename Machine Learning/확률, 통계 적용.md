# 딥러닝 면접 준비 - 통계/확률 이론 & 실무 가이드

## 1. 기초 통계 및 확률 이론

### 확률 이론의 기본 개념

**확률**은 어떤 사건이 일어날 가능성을 0과 1 사이의 수로 나타낸 것입니다. 0에 가까울수록 일어날 가능성이 낮고, 1에 가까울수록 높습니다.

**표본공간**은 실험에서 일어날 수 있는 모든 결과의 집합이고, **사건**은 표본공간의 부분집합입니다. 예를 들어 동전 던지기에서 표본공간은 {앞면, 뒷면}이고, "앞면이 나오는 사건"은 {앞면}입니다.

### 확률분포

**확률분포**는 확률변수가 취할 수 있는 값들과 그 확률들을 나타냅니다.

**이산확률분포**:
- 베르누이분포 (성공/실패)
- 이항분포 (n번 시행에서 성공 횟수)
- 포아송분포 (단위시간당 발생 횟수)

**연속확률분포**:
- 정규분포 (종 모양의 대칭분포)
- 지수분포 (대기시간 모델링)
- 균등분포 (모든 값이 동일한 확률)

### 기술통계학

**중심경향성 측도**:
- 평균 (산술평균)
- 중앙값 (전체 데이터의 중간값)
- 최빈값 (가장 자주 나타나는 값)

**산포 측도**:
- 범위 (최댓값-최솟값)
- 분산 (편차제곱의 평균)
- 표준편차 (분산의 제곱근)

### 추론통계학

**중심극한정리**: 표본 크기가 클 때 표본평균의 분포가 정규분포에 가까워진다는 중요한 이론

**신뢰구간**: 모집단 모수가 포함될 것으로 예상되는 구간을 확률적으로 표현

**가설검정**: 모집단에 대한 가설이 맞는지 통계적으로 판단하는 과정

## 2. 딥러닝에서 활용되는 통계 및 확률 이론

### 확률론적 모델링

딥러닝 모델은 본질적으로 **확률적 함수**입니다. 입력 데이터 x가 주어졌을 때 출력 y의 확률분포 P(y|x)를 학습합니다.

**베이즈 정리**: P(θ|D) ∝ P(D|θ)P(θ)에서 데이터 D가 주어졌을 때 모델 파라미터 θ의 사후확률을 구하는 데 사용

### 손실함수와 확률분포

**교차 엔트로피 손실함수**: 정보이론에서 나온 개념으로, 두 확률분포 간의 차이를 측정

**평균제곱오차(MSE)**: 가우시안 분포를 가정한 최대우도추정과 연결

### 정규화와 확률적 해석

**가중치 감소(Weight Decay)**: 통계학의 L2 정규화와 같으며, 가중치에 가우시안 사전분포를 가정

**드롭아웃**: 각 뉴런을 확률 p로 제거하는 정규화 기법으로, 베르누이 분포를 따르는 확률적 과정

### 불확실성 정량화

**인식론적 불확실성**: 모델의 불확실성
**무작위적 불확실성**: 데이터의 노이즈

**몬테카를로 드롭아웃**: 테스트 시에도 드롭아웃을 적용해 여러 번 예측한 결과의 분산으로 모델 불확실성을 추정

## 3. 면접에서 자주 나오는 질문들

### 확률 기초 개념

**Q: "확률과 조건부 확률의 차이를 설명해보세요"**
- P(A): 사건 A가 일어날 확률
- P(A|B): 사건 B가 주어졌을 때 A가 일어날 확률
- 베이즈 정리: P(A|B) = P(B|A)P(A) / P(B)
- 딥러닝에서 P(y|x) 형태로 입력 x가 주어졌을 때 출력 y의 확률을 모델링

**Q: "독립과 상관관계의 차이는?"**
- 독립: P(A∩B) = P(A)P(B), 한 사건이 다른 사건에 영향 없음
- 상관관계: 두 변수가 함께 변하는 정도 (-1 ≤ r ≤ 1)
- 독립이면 상관관계가 0이지만, 상관관계가 0이라고 독립은 아님

### 확률분포

**Q: "정규분포가 중요한 이유는?"**
- 중심극한정리로 인해 표본평균이 정규분포에 수렴
- 많은 자연현상이 정규분포를 따름
- 딥러닝에서 가중치 초기화, 노이즈 모델링에 사용

### 통계적 추론

**Q: "최대우도추정(MLE)을 설명해보세요"**
- 주어진 데이터에 대해 우도함수를 최대화하는 모수 추정
- 딥러닝의 손실함수 최소화와 MLE는 밀접한 관련
- 교차엔트로피 손실 = 음의 로그우도

**Q: "편향(bias)과 분산(variance)의 트레이드오프는?"**
- 편향: 예측값의 기댓값과 실제값의 차이
- 분산: 예측값들이 얼마나 퍼져있는지
- 총 오차 = 편향² + 분산 + 노이즈
- 모델 복잡도 ↑ → 편향 ↓, 분산 ↑

## 4. 프로젝트 경험자를 위한 쉬운 설명

### 확률의 "왜?"와 간단한 예시

**"왜 확률을 사용하나요?"**
- 고양이/강아지 분류 모델에서 "고양이일 확률 80%, 강아지일 확률 20%"처럼 불확실성을 표현
- 현실 세계는 불확실하기 때문

**조건부 확률이 왜 중요한가요?**
- "비가 올 때 우산을 들고 다닐 확률"과 "그냥 우산을 들고 다닐 확률"은 다름
- 딥러닝도 "이 이미지가 주어졌을 때 고양이일 확률"을 학습

### 손실함수의 "왜?"

**"왜 교차엔트로피를 쓰나요?"**
- 실제 답: 고양이(100%)
- 모델A 예측: 고양이 90%, 강아지 10%
- 모델B 예측: 고양이 60%, 강아지 40%
- 교차엔트로피는 모델A에게 더 좋은 점수를 줌 (확실하게 맞춘 것에 더 큰 보상)

**"왜 MSE는 회귀에 쓰나요?"**
- 집값 예측에서 큰 오차에 더 큰 패널티를 줌 (제곱하니까)
- 1억원 틀리는 게 1천만원 틀리는 것보다 훨씬 심각

### 과적합의 "왜?"

**"왜 과적합이 생기나요?"**
- 시험 기출문제만 달달 외운 학생과 같음
- 기출문제(훈련 데이터): 100점, 새로운 문제(테스트 데이터): 60점

**"왜 드롭아웃이 효과적인가요?"**
- 팀 프로젝트에서 한 명이 모든 일을 다 하면 그 사람이 없을 때 팀이 무너짐
- 뉴런 몇 개를 랜덤하게 "쉬게" 해서 모든 뉴런이 골고루 학습하도록 함

### 정규화의 "왜?"

**"왜 배치 정규화를 쓰나요?"**
- 요리할 때 재료마다 단위가 다르면 혼란스러움 (소금: 1티스푼, 밀가루: 200g, 물: 1컵)
- 모든 재료(데이터)를 비슷한 범위로 맞춰주면 모델이 학습하기 쉬워짐

## 5. Epoch와 Batch Size 설정 기준

### Batch Size 설정 기준

**메모리 제약 (가장 현실적인 이유)**
- 고해상도 이미지 처리 시 "CUDA out of memory" 에러 발생
- 배치 크기 = 한 번에 GPU 메모리에 올릴 수 있는 데이터 양

**학습 안정성**
- 배치 1 (SGD): 한 명의 의견만 듣고 결정 → 불안정
- 배치 32: 32명의 의견 평균내서 결정 → 안정적
- 배치 1000: 너무 많은 의견 → 학습 속도 느림

**실무 기준**:
- 시작점: 32 또는 64
- 메모리 부족하면: 16, 8로 줄이기
- 여유 있으면: 128, 256으로 늘려보기
- 최대: 보통 512까지

### Epoch 설정 기준

**조기 종료 (Early Stopping) 방식 - 추천!**
```
Epoch 1: 훈련 손실 0.8, 검증 손실 0.85
Epoch 5: 훈련 손실 0.3, 검증 손실 0.4
Epoch 10: 훈련 손실 0.1, 검증 손실 0.35 ← 최고점
Epoch 15: 훈련 손실 0.05, 검증 손실 0.45 ← 과적합 시작
```

**실무 접근법**:
1. 먼저 큰 수로 설정: 100~500 epoch
2. 조기 종료 설정: patience=10 (10 epoch 동안 개선 없으면 중단)
3. 모니터링: 검증 손실 그래프 관찰

### 실제 프로젝트에서의 설정 과정

**1단계: 하드웨어 체크**
```python
batch_size = 64
try:
    model.fit(X_train, y_train, batch_size=batch_size)
except:
    batch_size = 32  # 메모리 부족시 줄이기
```

**2단계: 빠른 실험**
```python
epochs = 20
model.fit(X_train, y_train, 
          batch_size=32, 
          epochs=epochs,
          validation_split=0.2)
```

**3단계: 본격 학습**
```python
early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
model.fit(X_train, y_train,
          batch_size=32,
          epochs=200,  # 충분히 크게 설정
          validation_split=0.2,
          callbacks=[early_stopping])
```

## 6. 면접 답변 팁

### 기본 원칙
1. 수식보다는 직관적 설명을 우선하되, 필요시 수식도 설명
2. 실제 딥러닝 프로젝트와 연결해서 설명
3. "왜?"라는 질문에 대한 답변 준비
4. 간단한 예시를 들어 설명

### 실무 연결 방법
- "모델 성능이 갑자기 떨어졌을 때" → 과적합 설명
- "학습이 불안정했을 때" → 정규화 설명
- "데이터가 부족했을 때" → 데이터 증강, 정규화 설명
- "클래스 불균형 문제" → 가중치 조정, 다양한 평가지표 설명

### 추천 답변 패턴
- "제가 프로젝트에서 이런 상황을 겪었는데..."
- "그때 이 방법을 써봤더니..."
- "원리를 찾아보니 통계적으로는..."

### 실무 꿀팁
- 새로운 데이터셋: 작은 batch(16-32)로 시작
- 안정적인 모델: 큰 batch(64-128) 사용
- 시간이 중요한 프로젝트: 조기 종료 patience를 5-10으로 설정
- 충분한 시간 있음: patience를 15-20으로 설정

## 7. 실무 프로젝트에서 통계/확률 이론 적용 사례

### 베이지안 추론의 실제 적용

**면접 질문**: "베이지안 추론을 실제 프로젝트에서 어떻게 사용해봤나요?"

**프로젝트 사례 - 음식량 측정 시스템**:
```python
# 색상 히스토그램 기반 역투영에서 베이지안 추론 적용
def back_projection(target_img, reference_img):
    """베이지안 추론을 이용한 음식 영역 검출"""
    
    # P(음식|색상) = P(색상|음식) * P(음식) / P(색상)
    
    # 1. 참조 이미지에서 음식의 색상 확률 분포 학습 (사전 확률)
    roi_hist = cv2.calcHist([hsv_reference], [0,1], None, [180,256], [0,180,0,256])
    cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)
    
    # 2. 새로운 이미지에서 각 픽셀이 음식일 사후 확률 계산
    dst = cv2.calcBackProject([hsv_target], [0,1], roi_hist, [0,180,0,256], 1)
    
    return dst
```

**답변 포인트**:
- "참조 이미지에서 음식의 색상 분포를 사전 확률로 학습했습니다"
- "새로운 이미지에서 해당 색상이 나타날 때 음식일 확률을 계산했어요"
- "단순한 색상 매칭보다 훨씬 정확한 음식 영역 검출이 가능했습니다"

### 로버스트 통계의 실무 활용

**면접 질문**: "이상치(outlier) 문제를 어떻게 해결했나요?"

**프로젝트 사례 - 깊이 맵 기반 3D 부피 측정**:
```python
def estimate_volume_with_robust_stats(depth_map):
    """백분위수를 이용한 robust한 부피 추정"""
    
    # 평균 대신 백분위수 사용으로 이상치에 robust하게 처리
    d_wall = np.percentile(depth_map, 95)    # 상위 5% (벽면)
    d_floor = np.percentile(depth_map, 5)    # 하위 5% (바닥)
    z_plane = np.percentile(masked_depth, 85) # 85th percentile 기준면
    
    # 통계적 스케일 추정
    scale_cm_per_unit = HEIGHT_CM / (d_wall - d_floor)
    
    return volume, scale_cm_per_unit
```

**답변 포인트**:
- "평균을 쓰면 노이즈나 극값에 영향받지만, 백분위수는 이상치에 robust해요"
- "95th/5th percentile로 벽면과 바닥을 구분해서 더 안정적인 결과를 얻었습니다"
- "실제로 MAE가 15% 개선되었어요"

### 확률적 모델 앙상블

**면접 질문**: "여러 모델을 결합할 때 어떤 방식을 사용했나요?"

**프로젝트 사례 - 적응적 가중치 앙상블**:
```python
def adaptive_ensemble_weights(backproj_result, resnet_confidence):
    """조건부 확률 기반의 적응적 가중치"""
    
    base_weights = {'backproj': 0.4, 'midas': 0.4, 'resnet': 0.2}
    
    # 베이지안 업데이트 개념 적용
    if resnet_confidence > 0.8:  # P(정확|신뢰도 높음)
        base_weights = {'backproj': 0.5, 'midas': 0.4, 'resnet': 0.1}
    elif resnet_confidence < 0.3:  # P(정확|신뢰도 낮음)
        base_weights = {'backproj': 0.6, 'midas': 0.3, 'resnet': 0.1}
    
    return base_weights

def weighted_ensemble(results, weights):
    """가중 평균을 통한 확률적 융합"""
    # 가중 평균 = Σ(wi * xi) where Σwi = 1
    return sum(result * weight for result, weight in zip(results, weights.values()))
```

**답변 포인트**:
- "단순한 평균이 아니라 조건부 확률 기반의 적응적 가중치를 사용했어요"
- "ResNet 신뢰도에 따라 베이지안 업데이트 개념으로 가중치를 동적 조정했습니다"
- "상황에 맞는 모델에 더 큰 가중치를 줘서 전체 성능이 향상됐어요"

### 다변량 확률 분포 활용

**면접 질문**: "고차원 데이터를 어떻게 처리했나요?"

**프로젝트 사례 - HSV 색공간의 결합 분포**:
```python
def multivariate_color_distribution(hsv_image, channels=(0,1)):
    """다변량 확률 분포로 색상 패턴 모델링"""
    
    # 2D 히스토그램으로 (색조, 채도)의 결합 확률 분포 구성
    hist_size = [180, 256]  # H: 0-180, S: 0-256
    ranges = [0, 180, 0, 256]
    
    # P(H, S) = 결합 확률 분포
    joint_hist = cv2.calcHist([hsv_image], [0,1], None, hist_size, ranges)
    
    # 확률 정규화
    total_pixels = hsv_image.shape[0] * hsv_image.shape[1]
    prob_distribution = joint_hist / total_pixels
    
    return prob_distribution
```

**답변 포인트**:
- "색상을 단일 채널이 아닌 H-S 채널의 결합 확률 분포로 모델링했어요"
- "2D 히스토그램으로 더 정확한 색상 패턴 매칭이 가능했습니다"
- "단일 채널 대비 F1-score가 12% 향상됐어요"

### 통계적 성능 평가

**면접 질문**: "모델 성능을 어떻게 정량적으로 평가했나요?"

**프로젝트 사례 - 다차원 성능 메트릭**:
```python
def comprehensive_performance_analysis(predictions, ground_truth):
    """통계적 성능 메트릭으로 모델 평가"""
    
    # 기술통계량 계산
    mae = np.mean(np.abs(predictions - ground_truth))
    rmse = np.sqrt(np.mean((predictions - ground_truth)**2))
    
    # 상관분석
    pearson_corr = np.corrcoef(predictions, ground_truth)[0,1]
    
    # 분포 분석
    error_distribution = predictions - ground_truth
    error_std = np.std(error_distribution)
    error_percentiles = np.percentile(error_distribution, [25, 50, 75])
    
    return {
        'MAE': mae, 'RMSE': rmse, 'Correlation': pearson_corr,
        'Error_Std': error_std, 'Error_Percentiles': error_percentiles
    }
```

**답변 포인트**:
- "MAE, RMSE로 예측 오차를 정량화했고, 상관계수로 선형 관계를 분석했어요"
- "오차 분포의 표준편차와 백분위수로 모델의 안정성을 평가했습니다"
- "단순히 정확도 하나만 보는 게 아니라 다각도로 성능을 분석했어요"

## 8. 면접에서 실무 경험 어필 전략

### 이론과 실무의 연결 강조

**"단순히 라이브러리를 쓴 게 아니라 왜 그 방법을 선택했는지 통계적 근거가 있었다"**

예시 답변:
- "처음엔 단순 평균을 썼는데 성능이 안 좋아서 데이터 분포를 관찰해봤어요"
- "이상치가 많다는 걸 발견하고 robust statistics를 찾아서 백분위수를 적용했습니다"
- "결과적으로 성능이 15% 개선됐고, 이론적 배경도 공부하게 됐어요"

### 문제 해결 과정에서의 통계적 사고

**"데이터를 보고 적절한 통계적 방법을 선택하는 과정"**

예시 답변:
- "히스토그램을 그려보니 정규분포가 아니라 편향된 분포였어요"
- "그래서 평균 대신 중앙값을 썼더니 더 안정적인 결과를 얻었습니다"
- "이런 경험을 통해 데이터의 성질을 먼저 파악하는 습관이 생겼어요"

### 성능 개선의 정량적 근거

**"A/B 테스트 개념으로 기존 방법과 비교 검증"**

예시 답변:
- "기존 방법과 새로운 방법을 동일한 테스트셋으로 비교했어요"
- "통계적으로 유의미한 개선인지 t-test로 검증했습니다"
- "단순히 '더 좋다'가 아니라 얼마나, 왜 좋은지 정량적으로 설명할 수 있어요"

### 예상 면접 질문 대응 전략

**Q: "통계 지식이 실제 개발에 어떤 도움이 되나요?"**
→ "음식량 측정 프로젝트에서 베이지안 추론을 써서 검출 정확도를 크게 높였어요"

**Q: "확률과 통계를 모르고도 딥러닝은 할 수 있지 않나요?"**
→ "기본적인 모델 사용은 가능하지만, 창의적 문제 해결에는 통계적 사고가 필수예요. 예를 들어 이상치 처리할 때..."

**Q: "가장 인상 깊었던 통계적 접근법은?"**
→ 베이지안 추론, 로버스트 통계, 다변량 분포 중 하나 선택해서 구체적 사례와 함께 설명

**Q: "실무에서 이론과 다른 점이 있었나요?"**
→ "이론에서는 정규분포를 가정하는데, 실제 데이터는 편향되어 있어서 robust한 방법이 필요했어요"