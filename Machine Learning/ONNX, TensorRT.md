# AI 모델 경량화 완전 가이드

## 📚 목차
1. [TensorRT와 ONNX 기초](#tensorrt와-onnx-기초)
2. [모델 경량화 개론](#모델-경량화-개론)
3. [최신 경량화 트렌드](#최신-경량화-트렌드)
4. [FP8과 4비트 이하 양자화](#fp8과-4비트-이하-양자화)
5. [실제 프로젝트 사례 분석](#실제-프로젝트-사례-분석)

---

## TensorRT와 ONNX 기초

### TensorRT vs ONNX: 쉬운 비유로 이해하기

#### ONNX (Open Neural Network Exchange)
**ONNX는 AI 모델의 "번역기" 역할**

- **카페 메뉴 비유**: 같은 "아메리카노"라도 스타벅스, 투썸플레이스, 이디야마다 레시피가 다르듯, PyTorch와 TensorFlow로 만든 모델은 서로 다른 "언어"로 되어 있음
- **공통 언어 역할**: 다양한 AI 프레임워크에서 만든 모델들을 하나의 표준 형식으로 변환
- **장점**: 
  - PyTorch에서 만든 모델을 TensorFlow에서 사용 가능
  - 개발할 때는 편한 도구를 쓰고, 서비스할 때는 다른 도구 사용 가능
  - 모델을 한 번 만들면 여러 환경에서 재사용 가능

#### TensorRT
**TensorRT는 AI 모델의 "성능 튜너" 역할**

- **자동차 튜닝샵 비유**: 같은 차라도 전문가가 엔진을 튜닝하면 더 빠르고 연료 효율이 좋아짐
- **최적화 기능**:
  - **속도 향상**: 계산 과정을 최적화해서 더 빠르게
  - **메모리 절약**: 더 적은 메모리 사용
  - **전력 효율**: 배터리 소모 줄임

### 실제 사용 예시

**게임 회사에서 AI 캐릭터 개발 시나리오:**
1. **개발 단계**: 연구팀이 PyTorch로 AI 모델 개발
2. **ONNX 변환**: 완성된 모델을 ONNX 형식으로 변환해서 다른 팀과 공유
3. **TensorRT 최적화**: 게임 엔진에 들어갈 때 TensorRT로 최적화해서 실시간 빠른 동작

### 간단한 비유 정리
- **ONNX**: 전 세계 어디서나 쓸 수 있는 "국제 표준 USB 케이블"
- **TensorRT**: 같은 기능이지만 "훨씬 빠른 고성능 케이블"

---

## 모델 경량화 개론

### 경량화가 필요한 이유

스마트폰이나 IoT 기기 같은 작은 장치에서 AI를 돌리려고 하면:
- **메모리 부족**: AI 모델이 너무 커서 작은 기기에 들어가지 않음
- **속도 느림**: 계산이 너무 복잡해서 결과가 늦게 나옴  
- **배터리 빨리 소모**: 전력을 너무 많이 사용

**경량화 목적**: 성능은 최대한 유지하면서 모델을 가볍고 빠르게 만들기

### 주요 경량화 방법들

#### 1. Network Architecture (건물 설계 바꾸기)
처음부터 **작고 효율적인 건물**을 설계하는 방법
- 요즘은 "만능 AI"가 인기라서 잘 안 쓰임

#### 2. Network Pruning (가지치기)
**나무 가지치기**처럼 중요하지 않은 부분을 잘라내는 방법
- 별로 중요하지 않은 연결이나 계산 부분을 제거
- 나무가 가지치기 후에도 잘 자라듯, 모델도 성능이 크게 떨어지지 않음

#### 3. Knowledge Distillation (선생님-학생 관계)
**큰 선생님 모델이 작은 학생 모델을 가르치는** 방법
- 선생님(큰 모델): "정답은 A야, 그런데 B도 30% 정도 가능성이 있어"
- 학생(작은 모델): 선생님의 자세한 설명을 듣고 똑똑해짐

#### 4. Matrix Decomposition (큰 짐을 작은 가방으로 나누기)
**하나의 큰 여행가방을 여러 개의 작은 가방으로 나누는** 것과 같음
- 큰 계산을 여러 개의 작은 계산으로 분해
- 전체 짐의 양은 같지만 옮기기 더 쉬워짐

#### 5. Network Quantization (고화질을 중화질로)
**4K 영상을 1080p로 압축하는** 것과 비슷
- 32비트 숫자 → 16비트 또는 8비트로 변환
- 화질은 조금 떨어지지만 파일 크기와 처리 속도는 훨씬 좋아짐

### 경량화 도구들

#### Apex: 훈련용 도구
**연습할 때 효율적으로 하는 방법**을 알려주는 도구
- Mixed Precision: 정밀한 계산과 빠른 계산을 적절히 섞어서 사용
- 시험공부할 때 중요한 부분은 꼼꼼히, 덜 중요한 부분은 빠르게 보는 것과 같음

#### ONNX: 번역기 (재설명)
**AI 모델의 구글 번역기** 역할
- **변환 과정의 두 가지 방법**:
  - **Tracing**: 실제로 한 번 실행해보면서 "이런 순서로 계산하는구나" 기록
  - **Script**: 코드 전체를 분석해서 "이런 계산들이 있구나" 파악

#### TensorRT: 성능 튜너 (재설명)
**자동차 튜닝샵**과 같음 (NVIDIA 전용)
- **Layer Fusion**: 여러 개의 작은 엔진을 하나의 큰 엔진으로 합침
- **Quantization**: 고급 연료 대신 일반 연료 사용 (속도↑, 효율↑)
- **GPU별 최적화**: 각 차종에 맞는 전용 튜닝

### 전체 과정 요약
1. **개발**: PyTorch로 AI 모델 개발
2. **변환**: ONNX로 다른 환경에서도 쓸 수 있게 변환  
3. **최적화**: TensorRT로 GPU에서 빠르게 돌도록 튜닝
4. **배포**: 작은 기기에서 빠르고 효율적으로 실행

---

## 최신 경량화 트렌드

### 🔥 2024-2025년 가장 HOT한 경량화 트렌드

#### 1. 고도화된 Quantization 기법들

**FP8 & 4비트 이하 양자화**가 대세:
- **NVFP4**: NVIDIA에서 새로 출시한 4비트 형식, 최대 4배 압축
- **FP8**: Mistral AI가 33% 성능 향상을 달성
- **1.58비트**: 연구진이 -1, 0, 1 값만으로 3B 파라미터 성능 달성

**대표적인 최신 양자화 방법들:**
- **AWQ (Activation-aware Weight Quantization)**: 중요한 가중치는 FP16으로 유지하고 나머지만 INT3/INT4로 양자화
- **GPTQ**: 4비트 양자화로 75-80% 모델 크기 감소, 정확도 손실 2% 미만
- **SmoothQuant**: 활성화와 가중치를 모두 8비트로 양자화

#### 2. 복합 기법의 조합 (Hybrid Approaches)

**가장 효과적인 접근법은 여러 기술을 조합하는 것**
- 가지치기 후 양자화를 적용하면 원본 대비 4-5배 작고 2-3배 빠른 모델 생성 가능

**인기 있는 조합:**
- **Pruning → Quantization**: 먼저 불필요한 부분 제거 후 양자화
- **Knowledge Distillation + Quantization**: 작은 학생 모델에 양자화까지 적용
- **Sparsity + Quantization**: 2:4 희소성과 양자화를 결합하여 1.5배 성능 향상

#### 3. 실시간 최적화 도구들의 발전

**통합 프레임워크들이 대세:**
- **NVIDIA TensorRT Model Optimizer**: 양자화, 가지치기, 증류, 희소성을 한 번에 적용
- **LLM Compressor**: vLLM과 통합되어 최대 4배 빠른 추론 실현

#### 4. Hardware-Aware Optimization

**특정 하드웨어에 최적화된 경량화:**
- Intel의 OpenVINO: Intel 하드웨어에서 3-8배 성능 향상
- **GPU별 맞춤 최적화**: NVIDIA Ampere GPU용 INT4, Hopper GPU용 FP8
- **모바일 최적화**: Stable Diffusion과 Llama 2를 스마트폰에서 실행

#### 5. Post-Training vs Quantization-Aware Training

**현재 트렌드는 Post-Training Quantization (PTQ)**:
- 빠르고 쉬운 배포를 위해 PTQ 방식이 인기
- 하지만 **QAT (Quantization-Aware Training)**도 정확도 유지를 위해 여전히 중요

### 🚀 실제 성능 개선 사례

**최신 벤치마크 결과들:**
- **Quantization**: 75-80% 크기 감소, 2% 미만 정확도 손실
- **Pruning**: 30-50% 파라미터 제거하면서 성능 유지
- **Knowledge Distillation**: 학생 모델이 선생님 모델의 90-95% 성능 달성

### 📱 Edge Device 중심의 발전

**모바일과 IoT 기기 대응이 핵심:**
- Qualcomm의 AIMET를 사용해 Stable Diffusion을 스마트폰에서 실행
- **실시간 추론**: 나노초 단위의 초고속 추론 달성
- **에너지 효율성**: 양자화된 모델의 전력 소비 대폭 감소

### 💡 개발자들이 실제로 사용하는 도구들

1. **GPTQ**: GPU 환경에서 가장 인기
2. **AWQ**: 정확도와 속도의 균형
3. **GGUF**: CPU 환경이나 Apple 기기용
4. **TensorRT**: NVIDIA GPU 전용 최고 성능

**결론**: 2024-2025년 트렌드는 "다양한 기법의 intelligent 조합"과 "하드웨어 특화 최적화"

---

## FP8과 4비트 이하 양자화

### 🔢 FP8 (8비트 부동소수점) 양자화

#### FP8이란?
**8비트로 표현되는 새로운 부동소수점 형식**
- 기존 INT8과 달리 **지수(exponent)와 가수(mantissa)를 분리**해서 더 넓은 범위의 값 표현

#### FP8의 구조와 종류
**주요 FP8 형식들:**
- **E5M2**: 5비트 지수 + 2비트 가수 (넓은 범위, 낮은 정밀도)
- **E4M3**: 4비트 지수 + 3비트 가수 (균형)  
- **E3M4**: 3비트 지수 + 4비트 가수 (좁은 범위, 높은 정밀도)

#### FP8의 핵심 장점

**1. 더 넓은 동적 범위**
- FP8은 지수 비트 덕분에 INT8보다 outlier(이상값)에 더 강함
- 특히 네트워크에 outlier가 많을 때 성능이 우수

**2. 실제 성능 개선**
- Neural Magic의 Llama 3.1 405B FP8 버전: 2배 이상 추론 속도 향상, 99.91% 정확도 유지
- 워크로드 커버리지: FP8 92.64% vs INT8 65.87%

**3. 하드웨어 최적화**
- NVIDIA GPU에서 FP8×FP8 행렬곱이 FP32로 누적되어 고속 처리

#### 실제 사용 사례

**Text-to-Image 모델:**
- Diffusion 모델에서 FP8 양자화 시 stochastic rounding 기법으로 bias를 줄여 full-precision과 구별되지 않는 이미지 생성

**NLP vs CV 모델:**
- E4M3는 NLP 모델에, E3M4는 컴퓨터 비전 작업에 더 적합

### 🔽 4비트 이하 초극한 양자화

#### 4비트 양자화의 종류

**1. 일반 INT4**
- 2^4=16개의 서로 다른 가중치만 사용
- 실제 산업에서 가장 인기 있는 양자화 방식

**2. NF4 (Normal Float 4)**
- 가중치가 정규분포를 따른다는 가정하에 0 근처에 더 많은 정밀도를 할당하는 비균등 양자화

**3. FP4 (4비트 부동소수점)**
- 1비트 부호 + 2비트 지수 + 1비트 가수로 구성
- INT4보다 넓은 동적 범위 제공

#### 혁신적인 4비트 이하 기술들

**1. 4.6비트 양자화**
- 4비트보다 정확하면서도 4비트와 거의 같은 계산 효율성(4% 느림)을 달성하는 새로운 방식

**2. 1.58비트 극한 양자화**
- 가중치를 -1, 0, 1 세 값만으로 표현하면서도 3B 파라미터 모델과 비슷한 성능 달성

#### 4비트 양자화의 실제 성과

**메모리 및 속도 개선:**
- 4비트 훈련으로 FP16 대비 7배 이상 하드웨어 가속 달성
- 4비트 post-training quantization으로 fine-tuning 없이도 baseline 대비 몇 % 내의 정확도 손실만 발생

**실용적 배포:**
- 4비트 양자화는 오픈소스 모델에서 가장 일반적으로 사용되는 양자화 방식

### 💡 FP8 vs 4비트 이하: 선택 기준

#### FP8를 선택해야 하는 경우:
- **정확도가 매우 중요**한 상용 서비스
- **NVIDIA GPU 환경**에서 최고 성능 필요
- **Outlier가 많은** 모델 (Transformer 등)
- **서버 환경**에서의 대규모 추론

#### 4비트 이하를 선택해야 하는 경우:
- **극도로 제한된 메모리** 환경 (모바일, IoT)
- **배터리 수명**이 중요한 기기
- **비용 절감**이 최우선인 대규모 배포
- **약간의 정확도 손실**을 감수할 수 있는 경우

### 🚀 미래 전망

**하드웨어 발전:**
- Google, Microsoft, Meta, ARM, Qualcomm 등이 8비트 이하 형식을 지원하는 차세대 하드웨어 개발 중

**기술 융합:**
- 최신 트렌드는 **FP8과 4비트를 상황에 따라 혼합 사용**
- 중요한 레이어는 FP8, 덜 중요한 부분은 4비트로 처리하는 **적응형 양자화**가 주목

---

## 실제 프로젝트 사례 분석

### 📊 급식 환경에서의 현실적 경량화 전략

#### 프로젝트 개요
- **환경**: 급식 관련 AI 서비스
- **제약**: 일반 사무용 컴퓨터, GPU 서버 없음, 제한된 예산
- **목표**: 5초 이내 처리, 90% 이상 정확도

#### ✅ ONNX 선택 - 똑똑한 판단!

**사용한 기술 스택:**
```python
# Requirements
onnxruntime>=1.14.0
onnx>=1.14.1

# ONNX 변환 구현
def convert_to_onnx(model, dummy_input, output_path):
    export(
        model, dummy_input, output_path,
        export_params=True, opset_version=12,
        do_constant_folding=True,
        input_names=['input'], output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}}
    )

# ONNX Runtime 추론
session = ort.InferenceSession(
    model_path,
    providers=['CPUExecutionProvider']  # CPU 전용
)
```

**실제 성능 결과:**
```python
v2_performance = {
    "processing_time": 2.3,    # 목표 5초 대비 2배 빠름
    "accuracy": 91.4,          # 목표 90% 초과 달성
    "uptime": 99.7,           # 높은 안정성
    "user_satisfaction": 94    # 사용자 만족도 우수
}
```

#### ❌ TensorRT를 사용하지 않은 이유

**현실적 제약사항:**
1. **환경적 제약**: GPU 서버 없음, 일반 PC 환경
2. **비용 제약**: GPU 구입비, 전력비, 냉각비 부담
3. **복잡도**: TensorRT 개발 및 운영 복잡도 높음

**TensorRT vs ONNX Runtime 비교:**

| 항목 | TensorRT | ONNX Runtime | 프로젝트 선택 |
|------|----------|--------------|--------------|
| **하드웨어** | NVIDIA GPU 필수 | CPU/GPU 모두 | CPU 환경 → ONNX |
| **성능** | 최고 (GPU) | 우수 (CPU) | 충분한 성능 |
| **복잡도** | 높음 | 낮음 | 단순함 우선 |
| **호환성** | NVIDIA 전용 | 크로스 플랫폼 | 범용성 필요 |
| **비용** | 높음 (GPU) | 낮음 (CPU) | 비용 효율성 |

### 💡 핵심 교훈들

#### 1. "최고 성능 ≠ 최적 선택"

많은 개발자들이 TensorRT나 최신 양자화 기법에 매력을 느끼지만, 실제로는:
- 사용 환경
- 예산
- 유지보수 난이도
- 팀의 기술 수준

모든 것을 고려해야 합니다.

#### 2. "적절한 최적화가 과도한 최적화보다 낫다"

이 프로젝트는 **68% 성능 향상**으로 목표를 달성했습니다. TensorRT로 더 빠르게 할 수 있었겠지만:
- 개발 시간 3배 증가
- 하드웨어 비용 10배 증가
- 운영 복잡도 5배 증가

**투자 대비 효과(ROI)**를 고려하면 ONNX가 정답이었습니다.

#### 3. 실제 프로덕션에서의 경량화 우선순위

```
1순위: 요구사항 달성 여부 (✅ 달성)
2순위: 개발/운영 비용 (✅ 저렴)
3순위: 팀 역량 (✅ 충분)
4순위: 최대 성능 (❌ 불필요)
```

### 🔄 업계 패턴 분석

#### 경량화 기술 선택 기준

**스타트업/중소기업:**
- ONNX Runtime (CPU) 또는 TensorFlow Lite
- 간단한 Quantization (FP16 → INT8)
- 비용과 단순함 우선

**대기업/유니콘:**
- TensorRT, FP8, AWQ 등 최신 기술
- 복잡한 하이브리드 최적화
- 성능 극한 추구

**이 프로젝트는 전형적인 "스타트업 패턴"**

#### 📈 단계별 발전 방향

만약 이 서비스가 성장한다면:

**점진적 업그레이드:**
1. **현재**: PyTorch → ONNX (CPU)
2. **성장기**: ONNX → TensorRT (GPU 도입)
3. **확장기**: FP8/INT4 도입으로 극한 최적화

### 🎯 최종 결론

**핵심 메시지:**
> "완벽한 최적화보다 적절한 최적화가 실제 비즈니스에서는 더 가치 있다"

**성공 요인:**
- 68% 성능 향상으로 목표 달성
- 단순한 구조로 유지보수성 확보
- 제한된 예산 내에서 최대 효과

**실무 교훈:**
최신 기술보다는 **주어진 제약 조건 하에서 최적의 선택**을 하는 것이 진짜 엔지니어링 역량입니다.

---

## 🔚 마무리

이 가이드는 AI 모델 경량화의 기초부터 최신 트렌드, 그리고 실제 프로덕션 환경에서의 현실적 고려사항까지 다루었습니다. 

**핵심은 항상 "상황에 맞는 최적의 선택"**입니다. 
- 기술적 완벽함보다는 비즈니스 목표 달성
- 최신 기술보다는 안정적이고 검증된 방법
- 과도한 최적화보다는 적절한 수준의 효율화
