## 주요 기술 용어 설명

### 1. SOTA (State-of-the-Art) 알고리즘

**설명:** SOTA는 "State-of-the-Art"의 약자로, 특정 분야 또는 특정 문제(예: 이미지 분류, 객체 탐지, 자연어 처리 등)에서 **현재까지 공개된 기술 중 가장 최고 성능을 달성한 알고리즘이나 모델**을 의미합니다. 학계나 산업계에서 연구 및 개발의 지표로 활용되며, 새로운 알고리즘이 발표될 때 기존 SOTA 모델과의 성능 비교를 통해 그 우수성을 입증합니다.

### 2. CNN (Convolutional Neural Network, 합성곱 신경망)

**설명:** CNN은 **주로 이미지나 영상과 같은 시각적 데이터를 분석하고 처리하는 데 사용되는 딥러닝 모델**입니다. 인간의 시각 시스템을 모방하여 설계되었으며, **합성곱 계층(Convolutional Layer)**과 **풀링 계층(Pooling Layer)**을 사용하여 이미지의 특징을 효과적으로 추출합니다. 이미지 내 객체의 위치가 바뀌어도(평행 이동) 인식할 수 있는 강점이 있습니다.

### 3. 모폴로지 연산 (Morphological Operations)

**설명:** 모폴로지 연산은 **이미지 내 객체의 형태, 크기, 구조 등을 분석하거나 변형하는 데 사용되는 컴퓨터 비전 기술**입니다. 주로 이진 이미지(흑백 이미지)에서 객체의 모양을 다듬거나 노이즈를 제거하는 데 활용됩니다.
* **주요 연산:**
    * **팽창 (Dilation):** 객체 영역을 확장하여 작은 구멍을 채우거나 끊어진 부분을 연결합니다.
    * **침식 (Erosion):** 객체 영역을 축소하여 노이즈를 제거하거나 객체의 경계를 부드럽게 만듭니다.
    * **열기 (Opening):** 침식 후 팽창을 수행하는 것으로, 작은 노이즈를 제거하면서 객체의 크기를 크게 바꾸지 않습니다.
    * **닫기 (Closing):** 팽창 후 침식을 수행하는 것으로, 객체 내부의 작은 구멍을 메우거나 가까운 객체를 연결합니다.

### 4. 스케일 캘리브레이션 (Scale Calibration)

**설명:** 컴퓨터 비전 분야에서 스케일 캘리브레이션은 **이미지 픽셀 단위의 측정값을 실제 물리적 세계의 단위(예: 센티미터, 미터)로 변환하는 과정**을 의미합니다. 카메라로 촬영된 이미지의 픽셀 간 거리가 실제 세계에서 얼마의 거리에 해당하는지 또는 깊이 추정 모델이 출력하는 추상적인 깊이 값을 실제 거리로 매핑하는 작업입니다. 실제 물체의 크기나 부피를 정확하게 측정하기 위해 필요합니다.

### 5. OMP/MKL 스레드

**설명:**
* **OMP (OpenMP):** C, C++, Fortran과 같은 언어에서 병렬 처리를 구현할 수 있도록 하는 멀티 프로세싱 프로그래밍 인터페이스입니다.
* **MKL (Intel Math Kernel Library):** 인텔에서 제공하는 수학 커널 라이브러리로, 선형 대수, 푸리에 변환 등 수학적 연산을 고성능으로 처리하기 위한 최적화된 루틴들을 포함합니다. NumPy, SciPy, PyTorch와 같은 파이썬 라이브러리들이 내부적으로 MKL을 활용하여 CPU 연산 성능을 높이는 경우가 많습니다.
이 둘은 모두 CPU에서 **다수의 코어를 활용하여 연산을 병렬화**함으로써 성능을 향상시키는 데 기여합니다.

### 6. OMP/MKL 스레드를 제한? (OMP/MKL 스레드 제한 이유)

**설명:** OMP/MKL 스레드를 제한하는 것은 **이 라이브러리들이 생성하는 스레드 수를 의도적으로 줄이는 것**을 의미합니다.
* **제한 이유:**
    * **CPU 코어 과부하 방지:** `ProcessPoolExecutor`와 같이 이미 프로세스 수준에서 병렬화를 수행하는 경우, 각 프로세스 내에서 OMP/MKL이 다시 여러 스레드를 생성하면 전체 시스템의 스레드 수가 CPU 코어 수를 초과하여 **과도한 컨텍스트 스위칭이 발생하고 오히려 성능 저하**를 일으킬 수 있습니다.
    * **메모리 오버헤드 감소:** 각 스레드는 고유의 스택과 리소스를 가지므로, 불필요하게 많은 스레드가 생성되면 메모리 사용량이 증가합니다. 스레드 수를 제한하면 **전체 시스템의 메모리 사용량을 최적화**할 수 있습니다.
    * **안정성 및 디버깅 용이성:** 너무 많은 스레드 간의 경쟁은 예측 불가능한 동작을 초래하거나 디버깅을 어렵게 만들 수 있습니다. 스레드 수를 명시적으로 제어함으로써 **시스템의 안정성을 높이고 문제를 예측하기 쉽게** 만듭니다.

### 7. Python의 GIL (Global Interpreter Lock)

**설명:** GIL은 "Global Interpreter Lock"의 약자로, **C-Python(가장 일반적으로 사용되는 파이썬 인터프리터)이 한 번에 하나의 스레드만 파이썬 바이트코드(Python bytecode)를 실행하도록 허용하는 메커니즘**입니다. 이 때문에 파이썬은 멀티스레딩을 사용하더라도 CPU 연산이 많은(CPU-bound) 작업에서는 **진정한 병렬 처리가 어렵습니다.**

### 8. ProcessPoolExecutor

**설명:** `ProcessPoolExecutor`는 파이썬의 `concurrent.futures` 모듈에 포함된 클래스로, **새로운 프로세스(process)들을 생성하여 작업을 병렬로 실행할 수 있도록 하는 도구**입니다. CPU 연산이 많은 작업을 여러 CPU 코어에 분산하여 처리할 때 매우 유용하며, 각 프로세스는 독립적인 파이썬 인터프리터를 가지므로 GIL 제약을 받지 않아 진정한 병렬 실행이 가능합니다.

### 9. ResNet (Residual Network, 잔차 네트워크)

**설명:** ResNet은 딥러닝 모델, 특히 CNN에서 **층(Layer)이 깊어질수록 발생할 수 있는 성능 저하 문제(Vanishing Gradient, Degradation problem)를 해결하기 위해 고안된 신경망 아키텍처**입니다. "잔차 블록(Residual Block)"이라는 특별한 구조를 사용하며, 입력값을 출력값에 직접 더해주는 **스킵 커넥션(Skip Connection)**을 포함하여 훨씬 깊은 네트워크를 안정적으로 훈련시키고 성능을 향상시킬 수 있습니다.

### 10. AsyncIO

**설명:** AsyncIO는 파이썬에서 **비동기(asynchronous) 프로그래밍을 위한 표준 라이브러리**입니다. `async` 및 `await` 키워드를 사용하여 동시성(concurrency) 코드를 작성할 수 있게 해줍니다. 주로 **I/O 바운드 작업(예: 네트워크 통신, 데이터베이스 접근, 파일 입출력)**에 적합하며, 한 작업이 I/O 대기로 인해 멈춰있을 때 CPU가 다른 작업을 수행할 수 있도록 스위칭하여 자원 낭비 없이 효율적으로 처리량을 높입니다.

### 11. aiohttp

**설명:** `aiohttp`는 **AsyncIO 기반의 비동기 HTTP 클라이언트/서버 프레임워크**입니다. AsyncIO와 함께 사용되어 웹 요청(HTTP 요청)을 비동기적으로 효율적으로 처리할 수 있도록 합니다. 여러 HTTP 요청을 동시에 보내고 응답을 기다리는 작업에서 탁월한 성능을 발휘합니다.

### 12. ONNX (Open Neural Network Exchange)

**설명:** ONNX는 "Open Neural Network Exchange"의 약자로, **다양한 딥러닝 프레임워크(예: PyTorch, TensorFlow, Keras)에서 학습된 모델을 상호 교환할 수 있도록 하는 개방형 표준 형식**입니다. 특정 프레임워크에 묶이지 않고, 모델을 한 프레임워크에서 학습한 후 다른 프레임워크나 런타임 환경에서 배포하고 실행할 수 있게 하여 모델 배포의 유연성을 높입니다.

### 13. ONNX Runtime

**설명:** ONNX Runtime은 **ONNX 형식으로 변환된 딥러닝 모델을 빠르고 효율적으로 실행하기 위한 고성능 추론(inference) 엔진**입니다. 다양한 하드웨어 가속기(CPU, GPU, FPGA 등)를 지원하며, 모델 추론 시 최적의 성능을 낼 수 있도록 자동으로 조율합니다.

### 14. CPUExecutionProvider

**설명:** `CPUExecutionProvider`는 **ONNX Runtime에서 모델 추론을 CPU에서 실행할 때 사용하는 실행 프로바이더(Execution Provider) 중 하나**입니다. GPU가 없거나, GPU 사용이 비효율적인 경우(예: 작은 배치 크기) CPU를 최적으로 활용하여 모델 추론을 수행합니다.

### 15. GAN (Generative Adversarial Network, 생성적 적대 신경망)

**설명:** GAN은 **서로 적대적으로 경쟁하는 두 개의 신경망(생성자, 판별자)을 통해 새로운 데이터를 생성하는 딥러닝 모델**입니다.
* **생성자 (Generator):** 실제 데이터와 유사한 가짜(fake) 데이터를 생성합니다.
* **판별자 (Discriminator):** 생성된 가짜 데이터와 실제 데이터를 구별하는 역할을 합니다.
이 경쟁적인 학습 과정을 통해 생성자는 점점 더 실제 같은 데이터를 만들어내고, 판별자는 더 정교하게 진짜와 가짜를 구별하는 능력을 키웁니다.

### 16. 이미지 증폭 방법론 (Image Augmentation Methods)

**설명:** 이미지 증폭(Image Augmentation)은 **원본 이미지 데이터를 변형하여 새로운 훈련 데이터를 인위적으로 생성하는 기법**입니다. 딥러닝 모델 훈련 시 데이터의 양이 부족하거나 모델이 특정 패턴에만 과도하게 학습되는 것을 방지하기 위해 사용됩니다.
* **주요 방법:** 뒤집기(Flipping), 회전(Rotation), 크기 조절(Resizing/Cropping), 색상 변환(Color Jittering), 노이즈 추가(Adding Noise), 블러링(Blurring) 등이 있습니다.
* **목적:** 모델의 일반화 성능을 향상시키고, 과적합을 줄이며, 데이터셋의 다양성을 확보합니다.

### 17. 과적합에 대한 우려 (Overfitting Concern)

**설명:** 과적합(Overfitting)은 **머신러닝 모델이 훈련 데이터에 너무 "과하게" 학습되어, 훈련 데이터에서는 높은 성능을 보이지만 실제로는 보지 못했던 새로운 데이터(테스트 데이터 또는 실제 환경 데이터)에서는 성능이 현저히 떨어지는 현상**을 의미합니다. 이는 모델 개발 단계에서 가장 경계해야 할 문제 중 하나입니다.
* **원인:** 훈련 데이터 양 부족, 모델의 복잡도 과도, 훈련 시간 과도 등이 있습니다.
* **해결 방법:** 이미지 증폭, 정규화(Regularization), 드롭아웃(Dropout), 조기 종료(Early Stopping), 더 많은 데이터 확보 등 다양한 방법으로 과적합을 방지합니다.
